# ğŸš¨ HÆ¯á»šNG DáºªN NHANH: Fix Model Training

Model hiá»‡n táº¡i **KHÃ”NG há»c Ä‘Æ°á»£c gÃ¬** (generate "Task 5" thay vÃ¬ "Task 2").

## TL;DR - LÃ m gÃ¬ bÃ¢y giá»?

**BÆ°á»›c 1: Cháº©n Ä‘oÃ¡n**
```bash
python mtup_v2/scripts/diagnose_model.py \
    --data_path data/train_mtup_unified.txt \
    --adapter_path outputs/mtup_260110/mtup_v2/final_adapter
```

**BÆ°á»›c 2: Train láº¡i vá»›i capacity cao hÆ¡n (RECOMMENDED)**
```bash
# XÃ³a model cÅ©
rm -rf outputs/mtup_260110/mtup_v2

# Train láº¡i
python mtup_v2/scripts/train_mtup_higher_capacity.py \
    --data_path data/train_mtup_unified.txt \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --output_dir outputs/mtup_260110/mtup_v2_rank64 \
    --epochs 20
```

Training sáº½ máº¥t khoáº£ng 3-4 giá».

**BÆ°á»›c 3: Test láº¡i**
```bash
python mtup_v2/scripts/debug_prediction.py \
    --adapter_path outputs/mtup_260110/mtup_v2_rank64/final_adapter \
    --test_sentence "bi ká»‹ch lÃ  á»Ÿ chá»— Ä‘Ã³ !"
```

---

## Chi tiáº¿t: Táº¡i sao model tháº¥t báº¡i?

### Hiá»‡n tÆ°á»£ng:
Model generate:
```
Task  1: (k ? / k :compound(chziaÃ…) :mode interrogative)
Task Ã¯Â¼': (Ã¯Â½â€¹ / k ? :compound(Ã¯Â½Æ’Ã¯Â½Ë†Ã¯Â¼'Ã¡ÂºÂ¥Ã¯Â½: interrogative)
```

### Váº¥n Ä‘á»:
1. âŒ "Task Ã¯Â¼'" thay vÃ¬ "Task 2" â†’ Model khÃ´ng há»c Ä‘Æ°á»£c task structure
2. âŒ Parse sai hoÃ n toÃ n â†’ Model khÃ´ng hiá»ƒu AMR parsing
3. âŒ CÃ³ thá»ƒ do LoRA rank quÃ¡ tháº¥p (32) cho task phá»©c táº¡p nÃ y

### Giáº£i phÃ¡p:
**TÄƒng model capacity** Ä‘á»ƒ há»c tá»‘t hÆ¡n:

| Parameter | Old Value | New Value | LÃ½ do |
|-----------|-----------|-----------|-------|
| LoRA rank | 32 | 64 | More parameters to learn complex patterns |
| LoRA alpha | 16 | 32 | Stronger LoRA adaptation |
| Learning rate | 5e-5 | 3e-5 | More stable, less likely to overshoot |
| Epochs | 15 | 20 | More training iterations |

---

## Checklist sau khi train xong

Sau khi train xong vá»›i `train_mtup_higher_capacity.py`, test báº±ng debug script:

```bash
python mtup_v2/scripts/debug_prediction.py \
    --adapter_path outputs/mtup_260110/mtup_v2_rank64/final_adapter \
    --test_sentence "bi ká»‹ch lÃ  á»Ÿ chá»— Ä‘Ã³ !"
```

### âœ… Output Ä‘Ãºng pháº£i cÃ³ dáº¡ng:
```
Task 1: (bi_ká»‹ch :domain(chá»— :mod(Ä‘Ã³)))
Task 2: (b / bi_ká»‹ch :domain(c / chá»— :mod(Ä‘ / Ä‘Ã³)))
```

### Kiá»ƒm tra:
- [ ] CÃ³ "Task 1:" vÃ  "Task 2:" (KHÃ”NG pháº£i "Task 5" hay "Task Ã¯Â¼'")
- [ ] Task 1 cÃ³ structure Ä‘Ãºng: `(bi_ká»‹ch ...)`
- [ ] Task 2 cÃ³ variables: `(b / bi_ká»‹ch ...)`
- [ ] Sá»‘ lÆ°á»£ng `(` báº±ng sá»‘ lÆ°á»£ng `)`

---

## Náº¿u váº«n tháº¥t báº¡i sau khi train vá»›i rank 64?

CÃ³ 3 kháº£ nÄƒng:

### 1. Data bá»‹ corrupt
```bash
# Kiá»ƒm tra encoding
file -i data/train_mtup_unified.txt

# Pháº£i lÃ : charset=utf-8
# Náº¿u khÃ´ng, re-download hoáº·c convert
```

### 2. Dataset quÃ¡ nhá» (1840 samples)
Cáº§n augment thÃªm data hoáº·c thá»­ pre-train approach:
- Train base model trÃªn large corpus trÆ°á»›c
- Fine-tune trÃªn task cá»¥ thá»ƒ sau

### 3. Base model khÃ´ng phÃ¹ há»£p
Thá»­ model khÃ¡c:
- `Qwen/Qwen2.5-14B-Instruct` (náº¿u cÃ³ Ä‘á»§ VRAM)
- `meta-llama/Llama-3.1-8B-Instruct`

---

## Gá»­i káº¿t quáº£ náº¿u cáº§n há»— trá»£

Sau khi cháº¡y diagnosis, gá»­i:
1. Output cá»§a `diagnose_model.py`
2. Loss progression (first vÃ  last loss)
3. Output cá»§a `debug_prediction.py` vá»›i model má»›i
