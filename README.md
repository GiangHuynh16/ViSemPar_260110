# Vietnamese AMR Parser - MTUP v2

Multi-Task Unified Prompting approach for Vietnamese Abstract Meaning Representation (AMR) parsing.

## ðŸŽ¯ Goal

Beat baseline F1 score (0.47) using a unified multi-task learning approach.

## ðŸ“š Quick Links

- **START HERE:** [MTUP_V2_QUICKSTART.md](MTUP_V2_QUICKSTART.md) - Quick start guide
- **FULL GUIDE:** [mtup_v2/docs/TRAINING_GUIDE.md](mtup_v2/docs/TRAINING_GUIDE.md) - Complete training guide
- **CONCEPT:** [mtup_v2/docs/MTUP_CONCEPT.md](mtup_v2/docs/MTUP_CONCEPT.md) - Understanding MTUP
- **EXAMPLES:** [mtup_v2/docs/COREFERENCE_EXAMPLES.md](mtup_v2/docs/COREFERENCE_EXAMPLES.md) - Co-reference handling
- **SUMMARY:** [MTUP_V2_SUMMARY.md](MTUP_V2_SUMMARY.md) - Implementation summary
- **SERVER SETUP:** [COPY_TO_SERVER.md](COPY_TO_SERVER.md) - Copy files to server

## ðŸš€ Quick Start

### 1. Preprocessing (Local)
```bash
python3 mtup_v2/preprocessing/create_mtup_data.py
```

### 2. Training (Server)
```bash
python3 mtup_v2/scripts/train_mtup_unified.py \
    --data_path data/train_mtup_unified.txt \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --output_dir outputs/mtup_v2 \
    --epochs 5
```

### 3. Prediction (Server)
```bash
python3 mtup_v2/scripts/predict_mtup_unified.py \
    --base_model Qwen/Qwen2.5-7B-Instruct \
    --adapter_path outputs/mtup_v2/final_adapter \
    --input_file data/public_test.txt \
    --output_file outputs/predictions.txt
```

### 4. Evaluation
```bash
python3 mtup_v2/scripts/evaluate.py \
    --predictions outputs/predictions.txt \
    --ground_truth data/public_test_ground_truth.txt
```

## ðŸ“‚ Project Structure

```
ViSemPar_new1/
â”œâ”€â”€ mtup_v2/                          # NEW: MTUP v2 implementation
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train_mtup_unified.py     # Training script (369 lines)
â”‚   â”‚   â”œâ”€â”€ predict_mtup_unified.py   # Prediction script (340 lines)
â”‚   â”‚   â””â”€â”€ evaluate.py               # Evaluation script (314 lines)
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ create_mtup_data.py       # Data preprocessing (241 lines)
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ README.md                 # Overview
â”‚       â”œâ”€â”€ MTUP_CONCEPT.md           # Concept explanation
â”‚       â”œâ”€â”€ TRAINING_GUIDE.md         # Detailed guide
â”‚       â””â”€â”€ COREFERENCE_EXAMPLES.md   # Co-reference examples
â”‚
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ mtup_v1/                      # OLD: Archived previous attempts
â”‚       â”œâ”€â”€ train_mtup*.py            # Old training scripts
â”‚       â””â”€â”€ *.md                      # Old documentation (92 files)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_mtup_unified.txt        # âœ… Generated: 1,262 samples
â”‚   â”œâ”€â”€ public_test.txt               # Test input
â”‚   â””â”€â”€ public_test_ground_truth.txt  # Ground truth
â”‚
â”œâ”€â”€ outputs/                          # Training outputs (created during training)
â”‚
â”œâ”€â”€ MTUP_V2_QUICKSTART.md            # â­ START HERE
â”œâ”€â”€ MTUP_V2_SUMMARY.md               # Implementation summary
â”œâ”€â”€ COPY_TO_SERVER.md                # Server setup guide
â””â”€â”€ README.md                        # This file
```

## ðŸ”‘ Key Concepts

### What is MTUP?

**MTUP** = **Multi-Task Unified Prompting**

- **1 MODEL** (not 2!)
- **1 PROMPT** (unified for both tasks)
- **2 TASKS** learned simultaneously:
  - Task 1: Vietnamese â†’ AMR Skeleton (no variables)
  - Task 2: Vietnamese â†’ Full AMR (with variables, PENMAN format)

### Why MTUP?

| Aspect | Pipeline (âŒ Wrong) | MTUP (âœ… Correct) |
|--------|-------------------|------------------|
| Models | 2 separate models | 1 unified model |
| Training | 2 training runs | 1 training run |
| Knowledge | Isolated | Shared learning |
| Efficiency | Lower | Higher |
| F1 Score | Baseline | Target: Better |

## ðŸ“Š Data Statistics

- **Training samples:** 1,262 (validated)
- **Data size:** 1.5 MB
- **Format:** Unified prompt with both tasks
- **Quality:** All samples validated for bracket balance

## âš™ï¸ Technical Details

### Model Configuration
- **Base Model:** Qwen/Qwen2.5-7B-Instruct
- **Method:** 4-bit QLoRA
- **LoRA Rank:** 64
- **Batch Size:** 2 (effective: 32 with gradient accumulation)
- **Learning Rate:** 1e-4
- **Epochs:** 5 (default)

### Hardware Requirements
- **GPU:** â‰¥24GB VRAM (RTX 3090/4090, A100)
- **RAM:** â‰¥32GB
- **Disk:** â‰¥50GB free
- **Training Time:** ~2-3 hours (5 epochs on RTX 4090)

## ðŸŽ“ Co-reference Handling

Critical for high F1 score!

### Rules:
1. **Define once:** `(t / tÃ´i)` - First occurrence
2. **Reuse:** `t` - Subsequent occurrences (NOT `(t / tÃ´i)` again!)
3. **Pronouns:** Must reference correct entity

### Example:
```
CÃ¢u: TÃ´i lÃ  bÃ¡c sÄ©. TÃ´i lÃ m á»Ÿ bá»‡nh viá»‡n.

âœ… CORRECT:
(a / and
    :op1(b / bÃ¡c_sÄ© :domain(t / tÃ´i))
    :op2(l / lÃ m :ARG0 t :location(b2 / bá»‡nh_viá»‡n)))
                    â†‘
                    Reuse variable 't'

âŒ WRONG:
:op2(l / lÃ m :ARG0(t / tÃ´i) ...)  â† Duplicate definition!
```

See [COREFERENCE_EXAMPLES.md](mtup_v2/docs/COREFERENCE_EXAMPLES.md) for more details.

## ðŸ“ˆ Expected Results

### Success Criteria:
- âœ… F1 > 0.47 (beat baseline)
- âœ… Valid PENMAN format
- âœ… Correct co-reference handling
- âœ… Balanced brackets

### Target Performance:
| Metric | Baseline | Target | Stretch |
|--------|----------|--------|---------|
| F1 Score | 0.47 | >0.47 | >0.50 |
| Improvement | - | +2% | +6% |

## ðŸ› ï¸ Troubleshooting

### Common Issues:

**1. Out of Memory (OOM)**
```python
# Edit train_mtup_unified.py
per_device_train_batch_size=1  # Reduce from 2
gradient_accumulation_steps=32  # Increase from 16
```

**2. No variables in predictions**
```bash
# Model needs more training
--epochs 10  # Increase from 5
```

**3. Duplicate node errors**
```bash
# Model hasn't learned co-reference well
# Check training data has co-reference examples
grep -A 5 "multi-sentence" data/train_mtup_unified.txt
```

See [TRAINING_GUIDE.md](mtup_v2/docs/TRAINING_GUIDE.md) for more solutions.

## ðŸ“ Files to Copy to Server

Essential files:
```bash
# Option 1: Copy directory
scp -r mtup_v2/ user@server:/path/to/ViSemPar_new1/
scp data/train_mtup_unified.txt user@server:/path/to/ViSemPar_new1/data/

# Option 2: Create tarball
tar -czf mtup_v2.tar.gz mtup_v2/ data/train_mtup_unified.txt
scp mtup_v2.tar.gz user@server:/path/to/ViSemPar_new1/
```

See [COPY_TO_SERVER.md](COPY_TO_SERVER.md) for detailed instructions.

## ðŸ”¬ Evaluation

The model generates predictions in PENMAN format, which are evaluated against ground truth using SMATCH metric:

- **Precision:** Correct triples / Predicted triples
- **Recall:** Correct triples / Gold triples
- **F1:** Harmonic mean of precision and recall

## ðŸ“š Documentation

### Essential Reading (in order):
1. [MTUP_V2_QUICKSTART.md](MTUP_V2_QUICKSTART.md) - Start here!
2. [mtup_v2/docs/MTUP_CONCEPT.md](mtup_v2/docs/MTUP_CONCEPT.md) - Understand the approach
3. [mtup_v2/docs/TRAINING_GUIDE.md](mtup_v2/docs/TRAINING_GUIDE.md) - Step-by-step guide
4. [mtup_v2/docs/COREFERENCE_EXAMPLES.md](mtup_v2/docs/COREFERENCE_EXAMPLES.md) - Critical for quality

### Reference:
- [MTUP_V2_SUMMARY.md](MTUP_V2_SUMMARY.md) - Implementation details
- [COPY_TO_SERVER.md](COPY_TO_SERVER.md) - Server setup

## ðŸŽ¯ Next Steps

1. âœ… Read [MTUP_V2_QUICKSTART.md](MTUP_V2_QUICKSTART.md)
2. âœ… Run preprocessing locally
3. âœ… Copy files to server (see [COPY_TO_SERVER.md](COPY_TO_SERVER.md))
4. âœ… Train on server
5. âœ… Evaluate results
6. ðŸŽ‰ Beat baseline F1!

## ðŸ“Š Version History

- **v2.0** (2026-01-10): Complete rewrite with unified MTUP approach
- **v1.x** (archived): Two-stage pipeline approach (incorrect MTUP)

## ðŸ† Goal

**Beat Baseline F1: 0.47 â†’ Target: >0.47 â†’ Stretch: >0.50**

---

**Status:** âœ… Ready for Training
**Last Updated:** 2026-01-10
**Author:** Vietnamese AMR Parsing Team
**Competition:** VLSP 2025 - AMR Parsing
