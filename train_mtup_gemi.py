import os
import argparse
import torch
import inspect
import re # Th√™m regex ƒë·ªÉ parse th√¥ng minh h∆°n
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from peft import LoraConfig
from trl import SFTTrainer

# ==========================================
# 1. TEMPLATE & PROMPTS
# ==========================================

def create_prompt_stage1(sentence, target_amr=None):
    sys_prompt = """B·∫°n l√† m·ªôt chuy√™n gia ng√¥n ng·ªØ h·ªçc v·ªÅ c·∫•u tr√∫c AMR (Abstract Meaning Representation).
Nhi·ªám v·ª•: Chuy·ªÉn ƒë·ªïi c√¢u ti·∫øng Vi·ªát ƒë·∫ßu v√†o sang ƒë·ªãnh d·∫°ng ƒë·ªì th·ªã AMR chu·∫©n PENMAN.
Y√™u c·∫ßu ƒë·∫∑c bi·ªát:
1. KH√îNG s·ª≠ d·ª•ng bi·∫øn (variables) ƒë·ªãnh danh (v√≠ d·ª•: kh√¥ng d√πng 't / t√¥i', ch·ªâ d√πng '(t√¥i)').
2. ƒê·∫£m b·∫£o c·∫•u tr√∫c ngo·∫∑c ƒë∆°n () c√¢n b·∫±ng ch√≠nh x√°c.
3. Ch·ªâ gi·ªØ l·∫°i c√°c Concept v√† Relation (v√≠ d·ª•: :ARG0, :ARG1, :mod).

V√≠ d·ª• m·∫´u:
Input: C·∫≠u b√© ƒëang ƒë·ªçc s√°ch.
Output: (ƒë·ªçc :ARG0 (c·∫≠u_b√©) :ARG1 (s√°ch))"""

    user_input = f"Input: {sentence}"
    prompt = f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    if target_amr:
        prompt += f"{target_amr}<|im_end|>"
    return prompt

def create_prompt_stage2(sentence, amr_no_vars, target_full_amr=None):
    sys_prompt = """B·∫°n l√† m·ªôt chuy√™n gia g√°n nh√£n d·ªØ li·ªáu AMR (Abstract Meaning Representation).
Nhi·ªám v·ª•: Ho√†n thi·ªán ƒë·ªì th·ªã AMR chu·∫©n PENMAN t·ª´ c·∫•u tr√∫c th√¥ (ch∆∞a c√≥ bi·∫øn) v√† c√¢u g·ªëc.

Y√™u c·∫ßu QUAN TR·ªåNG:
1. G√°n bi·∫øn (variables) ƒë·ªãnh danh cho m·ªói concept (vd: '(t√¥i)' -> '(t / t√¥i)').
2. T√ÅI S·ª¨ D·ª§NG BI·∫æN (Re-entrancy): N·∫øu m·ªôt ƒë·ªëi t∆∞·ª£ng xu·∫•t hi·ªán nhi·ªÅu l·∫ßn ho·∫∑c ƒë∆∞·ª£c thay th·∫ø b·∫±ng ƒë·∫°i t·ª´ (anh ·∫•y, n√≥, c·∫≠u ta...), h√£y d√πng l·∫°i bi·∫øn ƒë√£ khai b√°o tr∆∞·ªõc ƒë√≥ thay v√¨ t·∫°o bi·∫øn m·ªõi.
3. ƒê·∫£m b·∫£o ƒë√∫ng ƒë·ªãnh d·∫°ng PENMAN.

V√≠ d·ª• m·∫´u (Complex Re-entrancy):
Input: Nam c·ªë g·∫Øng h·ªçc b√†i v√¨ c·∫≠u ·∫•y mu·ªën ƒë·ªó. <sep> (c·ªë_g·∫Øng :ARG0 (Nam) :ARG1 (h·ªçc :ARG1 (b√†i)) :cause (mu·ªën :ARG0 (c·∫≠u_·∫•y) :ARG1 (ƒë·ªó)))
Output: (c / c·ªë_g·∫Øng
    :ARG0 (n / Nam)
    :ARG1 (h / h·ªçc
        :ARG0 n
        :ARG1 (b / b√†i))
    :cause (m / mu·ªën
        :ARG0 n
        :ARG1 (ƒë / ƒë·ªó
            :ARG0 n)))"""

    user_input = f"Input: {sentence} <sep> {amr_no_vars}"
    prompt = f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    if target_full_amr:
        prompt += f"{target_full_amr}<|im_end|>"
    return prompt

def format_data(sample, stage):
    text = sample['text'].strip()
    if not text: return None
    
    try:
        # --- C∆† CH·∫æ PARSE LINH HO·∫†T (H·ªó tr·ª£ c·∫£ format c≈© v√† m·ªõi) ---
        
        # STAGE 1
        if stage == 1:
            # Pattern 1: Format m·ªõi (SENT / AMR)
            match = re.search(r'SENT:\s*(.*?)\nAMR:\s*(.*)', text, re.DOTALL)
            if match:
                return create_prompt_stage1(match.group(1).strip(), match.group(2).strip())
            
            # Pattern 2: Format c≈© (Input / Output)
            match = re.search(r'Input:\s*(.*?)\nOutput:\s*(.*)', text, re.DOTALL)
            if match:
                return create_prompt_stage1(match.group(1).strip(), match.group(2).strip())

        # STAGE 2
        else:
            # Pattern 1: Format m·ªõi (SENT / NO_VAR / FULL)
            match = re.search(r'SENT:\s*(.*?)\nNO_VAR:\s*(.*?)\nFULL:\s*(.*)', text, re.DOTALL)
            if match:
                return create_prompt_stage2(match.group(1).strip(), match.group(2).strip(), match.group(3).strip())
            
            # Pattern 2: Format c≈© c√≥ <sep>
            # Input: sent <sep> no_var \n Output: full
            match = re.search(r'Input:\s*(.*?)<sep>(.*?)\nOutput:\s*(.*)', text, re.DOTALL)
            if match:
                return create_prompt_stage2(match.group(1).strip(), match.group(2).strip(), match.group(3).strip())

        # N·∫øu kh√¥ng match pattern n√†o
        return None
    except Exception:
        return None

# ==========================================
# 2. TRAINING SETUP
# ==========================================

def load_and_filter_dataset(file_path, stage):
    print(f"üìÇ Reading file: {file_path}")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Cannot find file: {file_path}")
        
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Split blocks
    blocks = content.strip().split('\n\n')
    blocks = [b for b in blocks if b.strip()]
    
    print(f"üìä Found {len(blocks)} raw blocks. Filtering...")
    
    valid_data = []
    failed_count = 0
    
    for i, b in enumerate(blocks):
        fmt = format_data({'text': b}, stage)
        if fmt: 
            valid_data.append(b)
        else:
            failed_count += 1
            # DEBUG: In ra l·ªói c·ªßa sample ƒë·∫ßu ti√™n ƒë·ªÉ user bi·∫øt ƒë∆∞·ªùng s·ª≠a
            if failed_count == 1:
                print(f"‚ö†Ô∏è  WARNING: Could not parse sample #{i+1}. It looks like this:\n---START---\n{b}\n---END---")
                print("üëâ Please check if 'SENT:' or 'Input:' keywords match the code logic.")

    print(f"‚úÖ Loaded: {len(valid_data)} valid samples. (Failed: {failed_count})")
    
    if len(valid_data) == 0:
        raise ValueError("‚ùå DATASET IS EMPTY! Please check the input file format or run prepare_data.py again.")
        
    return Dataset.from_dict({"text": valid_data})

def train(args):
    print(f"üöÄ START TRAINING STAGE {args.stage} | GPU 48GB Optimization")
    
    # Load dataset with debug info
    dataset = load_and_filter_dataset(args.data_path, args.stage)
    
    print("‚ú® GPU 48GB Detected: Loading model in BFloat16")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.bfloat16,       
        device_map="auto",
        attn_implementation="sdpa" 
    )
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    peft_config = LoraConfig(
        lora_alpha=64,
        lora_dropout=0.05,
        r=128,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=8,      
        gradient_accumulation_steps=4,      
        learning_rate=2e-4,
        weight_decay=0.01,
        bf16=True,       
        fp16=False,
        logging_steps=10,
        save_strategy="epoch",
        optim="adamw_torch", 
        report_to="none",
        remove_unused_columns=False, 
    )

    # Check for max_seq_length support
    trainer_kwargs = {
        "model": model,
        "train_dataset": dataset,
        "peft_config": peft_config,
        "processing_class": tokenizer,
        "args": training_args,
        "formatting_func": lambda batch: [format_data({'text': t}, args.stage) for t in batch['text']],
    }
    
    sig = inspect.signature(SFTTrainer.__init__)
    if 'max_seq_length' in sig.parameters:
        trainer_kwargs['max_seq_length'] = 2048
        trainer_kwargs['packing'] = False

    trainer = SFTTrainer(**trainer_kwargs)

    trainer.train()
    
    final_path = os.path.join(args.output_dir, "final_adapter")
    trainer.model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)
    print(f"‚úÖ Training Done. Saved to {final_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--stage", type=int, required=True)
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-7B-Instruct") 
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--epochs", type=int, default=5) 
    
    args = parser.parse_args()
    train(args)