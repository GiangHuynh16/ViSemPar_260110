#!/usr/bin/env python3
"""
Debug prediction to see raw model output.

Usage:
    python mtup_v2/scripts/debug_prediction.py \
        --adapter_path outputs/mtup_260110/mtup_v2/final_adapter \
        --test_sentence "bi k·ªãch l√† ·ªü ch·ªó ƒë√≥ !"
"""

import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def main(args):
    print("üîç DEBUG MODE - Checking raw model output\n")

    # Load model
    print(f"üì• Loading model...")

    try:
        import flash_attn
        attn_impl = "flash_attention_2" if torch.cuda.get_device_capability()[0] >= 8 else "sdpa"
    except ImportError:
        attn_impl = "sdpa"

    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation=attn_impl
    )

    model = PeftModel.from_pretrained(model, args.adapter_path)
    model.eval()

    tokenizer = AutoTokenizer.from_pretrained(
        args.base_model,
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token

    print(f"‚úÖ Model loaded\n")

    # Create prompt
    system_prompt = """B·∫°n l√† chuy√™n gia ph√¢n t√≠ch AMR (Abstract Meaning Representation) cho ti·∫øng Vi·ªát.
Nhi·ªám v·ª•: V·ªõi m·ªói c√¢u ti·∫øng Vi·ªát, sinh ra 2 output:

Task 1 - AMR Skeleton: C·∫•u tr√∫c AMR ch·ªâ c√≥ concept v√† relation, KH√îNG c√≥ bi·∫øn ƒë·ªãnh danh.
Task 2 - Full AMR: AMR ho√†n ch·ªânh v·ªõi bi·∫øn theo chu·∫©n PENMAN.

Quy t·∫Øc QUAN TR·ªåNG cho Task 2:
1. M·ªói concept ƒë·ªãnh nghƒ©a bi·∫øn M·ªòT l·∫ßn: (t / t√¥i)
2. T√°i s·ª≠ d·ª•ng bi·∫øn (co-reference): N·∫øu concept xu·∫•t hi·ªán l·∫°i, CH·ªà d√πng t√™n bi·∫øn, kh√¥ng vi·∫øt l·∫°i concept
   VD: :ARG0 (t / t√¥i) ... :ARG1 t  (KH√îNG ph·∫£i :ARG1 (t / t√¥i))
3. Bi·∫øn d√πng ch·ªØ c√°i ƒë·∫ßu: (t / t√¥i), (b / b√°c_sƒ©). N·∫øu tr√πng th√¨ th√™m s·ªë: (t2 / t√¥i)
4. ƒê·∫£m b·∫£o s·ªë ngo·∫∑c m·ªü ( b·∫±ng s·ªë ngo·∫∑c ƒë√≥ng )"""

    user_input = f"C√¢u: {args.test_sentence}"

    prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_input}<|im_end|>
<|im_start|>assistant
"""

    print("=" * 70)
    print("PROMPT:")
    print("=" * 70)
    print(prompt)
    print("=" * 70 + "\n")

    # Generate
    print("üöÄ Generating...\n")
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=False,
            repetition_penalty=1.2,
            no_repeat_ngram_size=3
        )

    # Decode
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)

    print("=" * 70)
    print("FULL GENERATED TEXT:")
    print("=" * 70)
    print(generated_text)
    print("=" * 70 + "\n")

    # Extract assistant part only
    if "<|im_start|>assistant" in generated_text:
        assistant_output = generated_text.split("<|im_start|>assistant")[-1]
        assistant_output = assistant_output.replace("<|im_end|>", "").strip()

        print("=" * 70)
        print("ASSISTANT OUTPUT ONLY:")
        print("=" * 70)
        print(assistant_output)
        print("=" * 70 + "\n")

        # Check for Task 1 and Task 2
        if "Task 1:" in assistant_output:
            print("‚úÖ Found Task 1")
        else:
            print("‚ùå Missing Task 1")

        if "Task 2:" in assistant_output:
            print("‚úÖ Found Task 2")
        else:
            print("‚ùå Missing Task 2")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Debug prediction output")

    parser.add_argument(
        "--base_model",
        type=str,
        default="Qwen/Qwen2.5-7B-Instruct",
        help="Base model name"
    )
    parser.add_argument(
        "--adapter_path",
        type=str,
        required=True,
        help="Path to adapter"
    )
    parser.add_argument(
        "--test_sentence",
        type=str,
        required=True,
        help="Test sentence"
    )

    args = parser.parse_args()
    main(args)
