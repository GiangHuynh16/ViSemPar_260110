# MTUP v2 - Training Guide

## Tá»•ng quan

HÆ°á»›ng dáº«n nÃ y sáº½ giÃºp báº¡n train model MTUP v2 tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i.

## YÃªu cáº§u há»‡ thá»‘ng

- GPU: â‰¥24GB VRAM (RTX 3090, 4090, A100, etc.)
- RAM: â‰¥32GB
- Disk: â‰¥50GB free space
- CUDA: â‰¥11.8
- Python: 3.8+

## CÃ i Ä‘áº·t

```bash
# Clone repo (if needed)
cd ViSemPar_new1

# Install dependencies
pip install torch transformers datasets peft accelerate bitsandbytes
pip install sentencepiece protobuf
pip install smatch  # For evaluation

# Optional: Install flash-attention-2 for faster training
pip install flash-attn --no-build-isolation
```

## Workflow hoÃ n chá»‰nh

### BÆ°á»›c 1: Preprocessing - Táº¡o Unified Data

```bash
python mtup_v2/preprocessing/create_mtup_data.py
```

**Output:** `data/train_mtup_unified.txt`

**Kiá»ƒm tra:**
```bash
# Xem sá»‘ samples
wc -l data/train_mtup_unified.txt

# Xem vÃ­ dá»¥ Ä‘áº§u tiÃªn
head -50 data/train_mtup_unified.txt
```

**Format mong Ä‘á»£i:**
```
<|im_start|>system
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch AMR...
<|im_end|>
<|im_start|>user
CÃ¢u: bi ká»‹ch lÃ  á»Ÿ chá»— Ä‘Ã³ !
<|im_end|>
<|im_start|>assistant
Task 1: (bi_ká»‹ch :domain(chá»— :mod(Ä‘Ã³)))
Task 2: (b / bi_ká»‹ch :domain(c / chá»— :mod(Ä‘ / Ä‘Ã³)))
<|im_end|>

<|im_start|>system
...
```

### BÆ°á»›c 2: Training

#### TrÃªn local machine (test)

```bash
python mtup_v2/scripts/train_mtup_unified.py \
    --data_path data/train_mtup_unified.txt \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --output_dir outputs/mtup_v2_unified_test \
    --epochs 1
```

#### TrÃªn server (production)

```bash
# SSH vÃ o server
ssh user@server

# Navigate to project
cd /path/to/ViSemPar_new1

# Activate environment
conda activate amr  # or your env name

# Run training
nohup python mtup_v2/scripts/train_mtup_unified.py \
    --data_path data/train_mtup_unified.txt \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --output_dir outputs/mtup_v2_unified_production \
    --epochs 5 \
    > logs/train_mtup_v2.log 2>&1 &

# Monitor progress
tail -f logs/train_mtup_v2.log

# Check GPU usage
nvidia-smi -l 1
```

**Training time estimate:**
- ~3000 samples
- 5 epochs
- Batch size 2, grad accumulation 16
- ~2-3 hours on RTX 4090

**Expected output:**
```
outputs/mtup_v2_unified_production/
â”œâ”€â”€ checkpoint-epoch-1/
â”œâ”€â”€ checkpoint-epoch-2/
â”œâ”€â”€ ...
â””â”€â”€ final_adapter/
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.safetensors
    â””â”€â”€ ...
```

### BÆ°á»›c 3: Prediction

```bash
python mtup_v2/scripts/predict_mtup_unified.py \
    --base_model Qwen/Qwen2.5-7B-Instruct \
    --adapter_path outputs/mtup_v2_unified_production/final_adapter \
    --input_file data/public_test.txt \
    --output_file outputs/predictions_mtup_v2.txt
```

**Kiá»ƒm tra output:**
```bash
# Xem sá»‘ predictions
wc -l outputs/predictions_mtup_v2.txt

# Xem 5 predictions Ä‘áº§u
head -5 outputs/predictions_mtup_v2.txt

# Check format (should have variables)
head -1 outputs/predictions_mtup_v2.txt | grep -o "/" | wc -l
# Should return > 0 (cÃ³ dáº¥u /)
```

### BÆ°á»›c 4: Evaluation

```bash
python mtup_v2/scripts/evaluate.py \
    --predictions outputs/predictions_mtup_v2.txt \
    --ground_truth data/public_test_ground_truth.txt \
    --output_comparison outputs/comparison_mtup_v2.txt
```

**Output:**
```
ğŸ“Š OFFICIAL SMATCH SCORES
==========================
   Precision: 0.5234 (52.34%)
   Recall:    0.4987 (49.87%)
   F1 Score:  0.5108 (51.08%)
==========================
```

**So sÃ¡nh vá»›i baseline:**
- Baseline F1: 0.47 (47%)
- MTUP v2 F1: ? (target > 0.47)

## Troubleshooting

### Issue 1: OOM (Out of Memory)

**Triá»‡u chá»©ng:**
```
RuntimeError: CUDA out of memory
```

**Giáº£i phÃ¡p:**
1. Giáº£m batch size:
```python
# In train_mtup_unified.py, line ~195
per_device_train_batch_size=1,  # Giáº£m tá»« 2 xuá»‘ng 1
gradient_accumulation_steps=32,  # TÄƒng tá»« 16 lÃªn 32
```

2. Giáº£m max_length:
```python
# In train_mtup_unified.py, line ~151
max_length=1536,  # Giáº£m tá»« 2048 xuá»‘ng 1536
```

3. Enable gradient checkpointing (Ä‘Ã£ báº­t máº·c Ä‘á»‹nh):
```python
gradient_checkpointing=True,
```

### Issue 2: Loss NaN

**Triá»‡u chá»©ng:**
```
Step 50: loss=nan
```

**Giáº£i phÃ¡p:**
1. Giáº£m learning rate:
```python
learning_rate=5e-5,  # Giáº£m tá»« 1e-4
```

2. Kiá»ƒm tra data:
```bash
# Check for empty/invalid AMRs
grep -n "(a / amr-empty)" data/train_mtup_unified.txt
```

### Issue 3: Model khÃ´ng há»c Task 2

**Triá»‡u chá»©ng:**
Predictions khÃ´ng cÃ³ biáº¿n (khÃ´ng cÃ³ dáº¥u `/`)

**Giáº£i phÃ¡p:**
1. Kiá»ƒm tra prompt masking:
```python
# In train_mtup_unified.py, line ~149
# Make sure labels are masked correctly
```

2. TÄƒng sá»‘ epochs:
```bash
--epochs 10  # TÄƒng tá»« 5 lÃªn 10
```

3. Kiá»ƒm tra training data:
```bash
# Should have both Task 1 and Task 2
grep "Task 2:" data/train_mtup_unified.txt | head -5
```

### Issue 4: Duplicate Node Error trong predictions

**Triá»‡u chá»©ng:**
```
Error: Duplicate node definition 't'
```

**Giáº£i phÃ¡p:**
ÄÃ¢y lÃ  váº¥n Ä‘á» model chÆ°a há»c tá»‘t co-reference. Cáº§n:
1. Train lÃ¢u hÆ¡n (more epochs)
2. TÄƒng LoRA rank:
```python
r=128,  # TÄƒng tá»« 64
```
3. ThÃªm nhiá»u examples vá» co-reference vÃ o training data

## Monitoring Training

### Metrics quan trá»ng

1. **Loss giáº£m dáº§n:**
```
Epoch 1: loss=2.5
Epoch 2: loss=1.8
Epoch 3: loss=1.3
...
```

2. **GPU Utilization:**
```bash
nvidia-smi
# Should see ~90-95% GPU usage
```

3. **Sample predictions during training:**
Thá»‰nh thoáº£ng test 1 sample Ä‘á»ƒ xem model há»c nhÆ° tháº¿ nÃ o:
```bash
# After each epoch
python mtup_v2/scripts/predict_mtup_unified.py \
    --adapter_path outputs/.../checkpoint-epoch-X \
    --input_file data/test_sample.txt \
    --output_file outputs/test_epoch_X.txt

# Compare outputs
cat outputs/test_epoch_*.txt
```

## Best Practices

### 1. Incremental Training
Train theo bÆ°á»›c:
- Epoch 1-2: Há»c cáº¥u trÃºc cÆ¡ báº£n
- Epoch 3-4: Há»c variable assignment
- Epoch 5+: Fine-tune co-reference

### 2. Data Validation
TrÆ°á»›c khi train, luÃ´n validate:
```bash
python mtup_v2/preprocessing/create_mtup_data.py
# Should show: "âœ… Valid samples: X/Y"
# X should be close to Y
```

### 3. Checkpoint Management
Giá»¯ checkpoints quan trá»ng:
```bash
# Copy best checkpoint
cp -r outputs/.../checkpoint-epoch-3 outputs/best_checkpoint
```

### 4. Experiment Tracking
Log táº¥t cáº£ experiments:
```bash
# Create experiment log
echo "Experiment: MTUP_v2_run1" > logs/experiments.txt
echo "Date: $(date)" >> logs/experiments.txt
echo "Config: epochs=5, lr=1e-4, r=64" >> logs/experiments.txt
echo "Result: F1=0.XX" >> logs/experiments.txt
echo "---" >> logs/experiments.txt
```

## Next Steps

Sau khi Ä‘áº¡t F1 > baseline:
1. Thá»­ cÃ¡c model size khÃ¡c (1.5B, 14B)
2. Thá»­ LoRA rank khÃ¡c (32, 128, 256)
3. Thá»­ learning rate khÃ¡c (5e-5, 2e-4)
4. Ensemble nhiá»u models
5. Post-processing rules

## LiÃªn há»‡

Náº¿u gáº·p váº¥n Ä‘á», check:
1. Logs: `logs/train_mtup_v2.log`
2. Archive: `archive/mtup_v1/` (old approaches)
3. Documentation: `mtup_v2/docs/`
