# ğŸŒ HuggingFace Deployment Guide

## ğŸ¯ Táº¡i Sao DÃ¹ng HuggingFace?

VÃ¬ báº¡n muá»‘n **build API á»Ÿ local**, khÃ´ng pháº£i server:

### âœ… HuggingFace (Recommended)
- Download vá» local trong **1-2 phÃºt**
- DÃ¹ng Ä‘Æ°á»£c **á»Ÿ báº¥t ká»³ Ä‘Ã¢u** (local, cloud, colab)
- **Automatic versioning**
- Dá»… share vá»›i team/reviewer
- Professional (nhÆ° cÃ¡c model SOTA khÃ¡c)

### âŒ LÆ°u TrÃªn Server
- Pháº£i scp download (~2-5 phÃºt)
- Chá»‰ access Ä‘Æ°á»£c khi cÃ³ SSH
- Manual versioning
- KhÃ³ share
- KhÃ´ng professional

## ğŸš€ Workflow HoÃ n Chá»‰nh

```
[SERVER]                    [HUGGINGFACE]              [LOCAL]

1. Train model    â†’    2. Push to HF Hub    â†’    3. Download & use
   (4-6h)                    (2-3 min)                 (1-2 min)

outputs/models/          your-username/           ~/.cache/huggingface/
mtup_two_task_7b/   â†’   vietnamese-amr-mtup  â†’   models/...
                                                    â†“
                                                 API Server
```

## ğŸ“‹ Complete Steps

### Step 1: Train on Server (HÃ´m Nay)

```bash
# On server
cd ~/ViSemPar_new1
git pull origin main

# Train MTUP
tmux new -s mtup_training
python3 train_mtup.py \
  --use-case best_accuracy \
  --epochs 10 \
  --output-dir outputs/models/mtup_two_task_7b
```

### Step 2: Setup HuggingFace (One-time)

```bash
# On server
pip install huggingface_hub

# Login to HF
huggingface-cli login
# Paste your token from: https://huggingface.co/settings/tokens
```

### Step 3: Push to HuggingFace (Sau Khi Train Xong)

```bash
# After training completes (~4-6h)
python3 push_to_huggingface.py \
  --model-path outputs/models/mtup_two_task_7b \
  --repo-name vietnamese-amr-mtup-7b \
  --model-type mtup \
  --private  # Use --private for private repo, omit for public
```

**Output**:
```
ğŸš€ Pushing MTUP model to HuggingFace Hub...
âœ… Logged in as: your-username
ğŸ“¦ Creating repository...
âœ… Repository created: your-username/vietnamese-amr-mtup-7b
ğŸ“¤ Uploading files...
âœ… SUCCESS!

ğŸ”— https://huggingface.co/your-username/vietnamese-amr-mtup-7b
```

### Step 4: Use on Local Machine (Your Laptop)

```bash
# On your local machine
pip install transformers peft torch

# No need to download manually - it auto-downloads!
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# Load from HuggingFace (downloads automatically to cache)
print("Loading model from HuggingFace...")

base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    device_map="auto",
    torch_dtype=torch.float16
)

# This downloads your model from HF
model = PeftModel.from_pretrained(
    base_model,
    "your-username/vietnamese-amr-mtup-7b"  # â† Your HF repo
)

tokenizer = AutoTokenizer.from_pretrained(
    "your-username/vietnamese-amr-mtup-7b"
)

print("âœ… Model ready for API!")
```

## ğŸŒ Build API on Local

### Option A: Simple Python API

```python
# api_local.py
from fastapi import FastAPI
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

app = FastAPI()

# Load model from HuggingFace
print("Loading model...")
base = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    device_map="auto",
    torch_dtype=torch.float16
)
model = PeftModel.from_pretrained(base, "your-username/vietnamese-amr-mtup-7b")
tokenizer = AutoTokenizer.from_pretrained("your-username/vietnamese-amr-mtup-7b")
print("âœ… Model loaded!")

@app.post("/parse")
async def parse(sentence: str):
    prompt = f"""### NHIá»†M Vá»¤
Chuyá»ƒn Ä‘á»•i cÃ¢u tiáº¿ng Viá»‡t sang AMR (2 bÆ°á»›c)

### CÃ‚U Äáº¦U VÃ€O
{sentence}

### Káº¾T QUáº¢

## BÆ¯á»šC 1: Cáº¥u trÃºc AMR (chÆ°a cÃ³ biáº¿n)
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_length=512)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract AMR
    if "AMR hoÃ n chá»‰nh:" in result:
        amr = result.split("AMR hoÃ n chá»‰nh:")[-1].strip()
    else:
        amr = result

    return {"sentence": sentence, "amr": amr}

# Run: uvicorn api_local:app --reload
```

### Option B: Gradio Web UI

```python
# gradio_ui.py
import gradio as gr
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load model
base = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    device_map="auto",
    torch_dtype=torch.float16
)
model = PeftModel.from_pretrained(base, "your-username/vietnamese-amr-mtup-7b")
tokenizer = AutoTokenizer.from_pretrained("your-username/vietnamese-amr-mtup-7b")

def parse_amr(sentence):
    prompt = f"""### NHIá»†M Vá»¤
Chuyá»ƒn Ä‘á»•i cÃ¢u tiáº¿ng Viá»‡t sang AMR (2 bÆ°á»›c)

### CÃ‚U Äáº¦U VÃ€O
{sentence}

### Káº¾T QUáº¢

## BÆ¯á»šC 1: Cáº¥u trÃºc AMR (chÆ°a cÃ³ biáº¿n)
"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_length=512)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result

# Create UI
demo = gr.Interface(
    fn=parse_amr,
    inputs=gr.Textbox(label="Vietnamese Sentence", lines=2),
    outputs=gr.Textbox(label="AMR Output", lines=10),
    title="Vietnamese AMR Parser",
    description="Parse Vietnamese sentences to Abstract Meaning Representation"
)

demo.launch()
```

## ğŸ“Š Timeline & Storage

### Timeline
1. **Training** (server): 4-6 hours
2. **Push to HF** (server): 2-3 minutes
3. **Download to local**: 1-2 minutes (first time only)
4. **API ready**: Immediate (cached)

### Storage

**On Server** (temporary):
```
outputs/models/mtup_two_task_7b/  (~500MB)
```
â†’ Can delete after pushing to HF!

**On HuggingFace** (permanent):
```
your-username/vietnamese-amr-mtup-7b  (~500MB)
```

**On Local** (cached):
```
~/.cache/huggingface/hub/models--your-username--vietnamese-amr-mtup-7b/
```
â†’ Downloaded once, reused forever!

## ğŸ” Private vs Public

### Private Repo (Recommended for thesis)
```bash
python3 push_to_huggingface.py \
  --model-path outputs/models/mtup_two_task_7b \
  --repo-name vietnamese-amr-mtup-7b \
  --private  # â† Add this flag
```

- âœ… Only you can access
- âœ… Can share with specific people (add collaborators on HF)
- âœ… Good for thesis work before publication

### Public Repo (After thesis)
```bash
python3 push_to_huggingface.py \
  --model-path outputs/models/mtup_two_task_7b \
  --repo-name vietnamese-amr-mtup-7b
  # No --private flag = public
```

- âœ… Anyone can use
- âœ… Good for citations
- âœ… Contributes to community

## ğŸ¯ Complete Workflow Example

### Day 1: Train MTUP
```bash
# On server
cd ~/ViSemPar_new1
git pull origin main

tmux new -s mtup
python3 train_mtup.py --use-case best_accuracy --output-dir outputs/models/mtup_two_task_7b
# Wait 4-6 hours...
```

### Day 1 (Evening): Push to HF
```bash
# After training completes
huggingface-cli login  # One-time setup

python3 push_to_huggingface.py \
  --model-path outputs/models/mtup_two_task_7b \
  --repo-name vietnamese-amr-mtup-7b \
  --model-type mtup \
  --private

# âœ… Done! Model on HF: https://huggingface.co/your-username/vietnamese-amr-mtup-7b
```

### Day 2: Train Baseline
```bash
# Same process for baseline
python3 train_baseline.py --output-dir outputs/models/baseline_single_task_7b

# Push to HF
python3 push_to_huggingface.py \
  --model-path outputs/models/baseline_single_task_7b \
  --repo-name vietnamese-amr-baseline-7b \
  --model-type baseline \
  --private
```

### Day 3: Build API on Local
```bash
# On your laptop
pip install transformers peft torch gradio

# Run Gradio UI
python gradio_ui.py
# Opens browser at http://localhost:7860
```

## ğŸ” Verify Model on HuggingFace

After pushing, check:

1. **Go to**: `https://huggingface.co/your-username/vietnamese-amr-mtup-7b`

2. **Should see**:
   - âœ… Model card (README)
   - âœ… Files: `adapter_model.bin`, `adapter_config.json`, etc.
   - âœ… Model size: ~400-600MB

3. **Test download**:
```python
from peft import PeftModel

# This should work immediately
model = PeftModel.from_pretrained(
    base_model,
    "your-username/vietnamese-amr-mtup-7b"
)
```

## ğŸ¯ Summary

**Question**: LÆ°u model á»Ÿ server hay push HF?
**Answer**: **Push lÃªn HuggingFace** âœ…

**Why**:
- âœ… DÃ¹ng dá»… dÃ ng trÃªn local (1 dÃ²ng code)
- âœ… Professional & portable
- âœ… Automatic versioning
- âœ… Can share with reviewer/team
- âœ… No need SSH to server

**How**:
1. Train trÃªn server
2. Push lÃªn HF (`push_to_huggingface.py`)
3. Use trÃªn local (`from_pretrained("your-repo")`)

**Time**:
- Push: 2-3 phÃºt
- Download (first time): 1-2 phÃºt
- After that: Instant (cached)

---

**Ready to deploy!** ğŸš€
