# ðŸš€ Training MTUP First - Clear Organization

## ðŸŽ¯ Plan

**HÃ´m nay**: Train MTUP (Qwen 2.5 7B, 2-task decomposition)
**NgÃ y mai**: Train Baseline (Qwen 2.5 7B, single-task)

## ðŸ“ Folder Structure (RÃµ RÃ ng)

```
ViSemPar_new1/
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/                              # â† Main models folder
â”‚   â”‚   â”œâ”€â”€ baseline_single_task_7b/        # â† Baseline (tomorrow)
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â”‚   â””â”€â”€ training_args.bin
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ mtup_two_task_7b/               # â† MTUP (today)
â”‚   â”‚       â”œâ”€â”€ adapter_model.bin
â”‚   â”‚       â”œâ”€â”€ adapter_config.json
â”‚   â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚       â””â”€â”€ training_args.bin
â”‚   â”‚
â”‚   â”œâ”€â”€ checkpoints_mtup/                    # Training checkpoints (temp)
â”‚   â”‚   â”œâ”€â”€ checkpoint-250/
â”‚   â”‚   â”œâ”€â”€ checkpoint-500/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/                          # Evaluation results
â”‚       â”œâ”€â”€ mtup_results.json
â”‚       â””â”€â”€ baseline_results.json
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ training_mtup.log
â”‚   â””â”€â”€ training_baseline.log
â”‚
â””â”€â”€ api/                                      # â† For API deployment (future)
    â”œâ”€â”€ load_model.py
    â”œâ”€â”€ api_server.py
    â””â”€â”€ README.md
```

## ðŸ”§ Step 1: Update Config for Clear Folder Names

TÃ´i sáº½ update config Ä‘á»ƒ lÆ°u vÃ o folder rÃµ rÃ ng hÆ¡n:

### config_mtup.py
```python
# Change line 14:
CHECKPOINT_DIR = OUTPUT_DIR / "checkpoints_mtup"

# To:
CHECKPOINT_DIR = OUTPUT_DIR / "models/mtup_two_task_7b"
```

### config.py (for baseline - tomorrow)
```python
# Change line 14:
CHECKPOINT_DIR = OUTPUT_DIR / "checkpoints"

# To:
CHECKPOINT_DIR = OUTPUT_DIR / "models/baseline_single_task_7b"
```

## ðŸš€ Step 2: Train MTUP Today

### Create folders
```bash
cd ~/ViSemPar_new1

# Create clean folder structure
mkdir -p outputs/models/mtup_two_task_7b
mkdir -p outputs/models/baseline_single_task_7b
mkdir -p outputs/evaluation
mkdir -p logs
mkdir -p api
```

### Pull latest code
```bash
git stash
git pull origin main
git stash drop
```

### Train MTUP
```bash
# Start training in tmux
tmux new -s mtup_training

# Train with clear output path
python3 train_mtup.py \
  --use-case best_accuracy \
  --epochs 10 \
  --output-dir outputs/models/mtup_two_task_7b

# Detach: Ctrl+B, then D
```

### Monitor training
```bash
# Watch log
tail -f logs/training_mtup.log

# Check GPU
watch -n 1 nvidia-smi

# Re-attach
tmux attach -t mtup_training
```

## ðŸ“Š Step 3: After Training - Save Final Model

Training sáº½ tá»± Ä‘á»™ng save vÃ o:
```
outputs/models/mtup_two_task_7b/
â”œâ”€â”€ adapter_model.bin          # LoRA weights (~400MB)
â”œâ”€â”€ adapter_config.json        # LoRA config
â”œâ”€â”€ tokenizer_config.json      # Tokenizer
â”œâ”€â”€ special_tokens_map.json    # Special tokens
â”œâ”€â”€ training_args.bin          # Training args
â””â”€â”€ trainer_state.json         # Training state
```

## ðŸ” Step 4: Verify Model After Training

```bash
# Check model exists
ls -lh outputs/models/mtup_two_task_7b/

# Check size
du -sh outputs/models/mtup_two_task_7b/
# Should be ~400-600MB (LoRA adapters only)

# Test load
python3 -c "
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

print('Loading base model...')
base = AutoModelForCausalLM.from_pretrained(
    'Qwen/Qwen2.5-7B-Instruct',
    device_map='auto',
    torch_dtype=torch.float16
)

print('Loading MTUP adapter...')
model = PeftModel.from_pretrained(
    base,
    'outputs/models/mtup_two_task_7b'
)

print('Loading tokenizer...')
tokenizer = AutoTokenizer.from_pretrained(
    'outputs/models/mtup_two_task_7b'
)

print('âœ… MTUP model loaded successfully!')
print(f'Model device: {model.device}')
"
```

## ðŸ“Š Step 5: Evaluate MTUP

```bash
# Evaluate on test set
python3 evaluate_mtup_model.py \
  --checkpoint outputs/models/mtup_two_task_7b \
  --test-file data/public_test_ground_truth.txt \
  --output outputs/evaluation/mtup_results.json

# Results will show F1, Precision, Recall
```

## ðŸŒ Step 6: Prepare for API (Tomorrow)

### Create API loader script
```bash
cat > api/load_model.py << 'EOF'
"""
Load trained Vietnamese AMR models for API deployment
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from pathlib import Path

class AMRModelLoader:
    """Load and manage AMR models"""

    def __init__(self, model_type='mtup'):
        """
        Args:
            model_type: 'mtup' or 'baseline'
        """
        self.model_type = model_type
        self.base_model_name = "Qwen/Qwen2.5-7B-Instruct"

        # Model paths
        if model_type == 'mtup':
            self.adapter_path = "outputs/models/mtup_two_task_7b"
        else:
            self.adapter_path = "outputs/models/baseline_single_task_7b"

    def load(self, device='auto'):
        """Load model and tokenizer"""
        print(f"Loading {self.model_type} model...")

        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            device_map=device,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )

        # Load LoRA adapter
        model = PeftModel.from_pretrained(base_model, self.adapter_path)
        model.eval()

        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.adapter_path)

        print(f"âœ… {self.model_type.upper()} model loaded")
        return model, tokenizer

    def generate_amr(self, model, tokenizer, sentence, max_length=512):
        """Generate AMR for a sentence"""

        if self.model_type == 'mtup':
            # MTUP: 2-task prompt
            prompt = f"""### NHIá»†M Vá»¤
Chuyá»ƒn Ä‘á»•i cÃ¢u tiáº¿ng Viá»‡t sang AMR (2 bÆ°á»›c)

### CÃ‚U Äáº¦U VÃ€O
{sentence}

### Káº¾T QUáº¢

## BÆ¯á»šC 1: Cáº¥u trÃºc AMR (chÆ°a cÃ³ biáº¿n)
"""
        else:
            # Baseline: Simple prompt
            prompt = f"""Convert the following Vietnamese sentence to AMR format.

Sentence: {sentence}

AMR:
"""

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                do_sample=False,
                num_beams=1,
                pad_token_id=tokenizer.eos_token_id
            )

        result = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract AMR
        if self.model_type == 'mtup':
            # Extract from "BÆ¯á»šC 2" section
            if "## BÆ¯á»šC 2" in result:
                parts = result.split("## BÆ¯á»šC 2")[1]
                if "AMR hoÃ n chá»‰nh:" in parts:
                    amr = parts.split("AMR hoÃ n chá»‰nh:")[-1].strip()
                else:
                    amr = parts.strip()
            else:
                amr = result.strip()
        else:
            # Extract after prompt
            amr = result.replace(prompt, "").strip()

        # Clean: Extract AMR structure only
        if '(' in amr:
            first_paren = amr.index('(')
            amr = amr[first_paren:].strip()

        return amr


# Usage example
if __name__ == "__main__":
    # Load MTUP model
    loader = AMRModelLoader('mtup')
    model, tokenizer = loader.load()

    # Test
    sentence = "TÃ´i yÃªu Viá»‡t Nam"
    amr = loader.generate_amr(model, tokenizer, sentence)

    print(f"Sentence: {sentence}")
    print(f"AMR: {amr}")
EOF
```

### Create simple API server (for future)
```bash
cat > api/api_server.py << 'EOF'
"""
Simple FastAPI server for Vietnamese AMR parsing
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from load_model import AMRModelLoader
import uvicorn

# Initialize
app = FastAPI(title="Vietnamese AMR Parser API")

# Load model on startup
print("Loading MTUP model...")
loader = AMRModelLoader('mtup')
model, tokenizer = loader.load()
print("âœ… Model ready")

class AMRRequest(BaseModel):
    sentence: str
    max_length: int = 512

class AMRResponse(BaseModel):
    sentence: str
    amr: str
    model_type: str

@app.post("/parse", response_model=AMRResponse)
async def parse_sentence(request: AMRRequest):
    """Parse Vietnamese sentence to AMR"""
    try:
        amr = loader.generate_amr(
            model,
            tokenizer,
            request.sentence,
            max_length=request.max_length
        )

        return AMRResponse(
            sentence=request.sentence,
            amr=amr,
            model_type="mtup_two_task_7b"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "ok", "model": "mtup_two_task_7b"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOF
```

### Create API README
```bash
cat > api/README.md << 'EOF'
# Vietnamese AMR Parser API

## Models Available

1. **MTUP (Two-Task)**: `mtup_two_task_7b`
   - Location: `outputs/models/mtup_two_task_7b/`
   - Approach: 2-task decomposition
   - Expected F1: ~0.49-0.53

2. **Baseline (Single-Task)**: `baseline_single_task_7b`
   - Location: `outputs/models/baseline_single_task_7b/`
   - Approach: Direct generation
   - Expected F1: ~0.42-0.46

## Quick Start

### Load Model Programmatically

```python
from api.load_model import AMRModelLoader

# Load MTUP
loader = AMRModelLoader('mtup')
model, tokenizer = loader.load()

# Parse sentence
amr = loader.generate_amr(model, tokenizer, "TÃ´i yÃªu Viá»‡t Nam")
print(amr)
```

### Run API Server

```bash
# Install dependencies
pip install fastapi uvicorn

# Start server
cd api
python api_server.py

# Server runs on http://localhost:8000
```

### API Usage

```bash
# Health check
curl http://localhost:8000/health

# Parse sentence
curl -X POST http://localhost:8000/parse \
  -H "Content-Type: application/json" \
  -d '{"sentence": "TÃ´i yÃªu Viá»‡t Nam"}'
```

## Model Paths

Both models are LoRA adapters (~400-600MB each):

```
outputs/models/
â”œâ”€â”€ mtup_two_task_7b/          # MTUP model
â”‚   â””â”€â”€ adapter_model.bin
â””â”€â”€ baseline_single_task_7b/   # Baseline model
    â””â”€â”€ adapter_model.bin
```

Base model (Qwen 2.5 7B) will be downloaded automatically from HuggingFace.
EOF
```

## ðŸ“‹ Summary

### Today's Tasks (MTUP Training)

1. âœ… Create clear folder structure
2. âœ… Pull latest code
3. ðŸ”„ Train MTUP â†’ `outputs/models/mtup_two_task_7b/`
4. âœ… Verify model loads correctly
5. âœ… Evaluate on test set
6. âœ… Create API loader for future use

### Tomorrow's Tasks (Baseline Training)

1. Train Baseline â†’ `outputs/models/baseline_single_task_7b/`
2. Evaluate baseline
3. Compare MTUP vs Baseline
4. Document results for thesis

### For API Deployment (Future)

Models are ready to use at:
- **MTUP**: `outputs/models/mtup_two_task_7b/`
- **Baseline**: `outputs/models/baseline_single_task_7b/`

Just use `api/load_model.py` to load them! ðŸš€

## ðŸš€ Quick Start Command

```bash
# On server - Today
cd ~/ViSemPar_new1
git stash && git pull origin main && git stash drop
mkdir -p outputs/models/{mtup_two_task_7b,baseline_single_task_7b} outputs/evaluation logs api

# Train MTUP
tmux new -s mtup_training
python3 train_mtup.py \
  --use-case best_accuracy \
  --epochs 10 \
  --output-dir outputs/models/mtup_two_task_7b
```

Expected time: **4-6 hours** â°

---

**Status**: Ready to train MTUP! ðŸŽ¯
**Folder**: Clear and organized for API use âœ…
**Next**: Baseline training tomorrow ðŸ“…
