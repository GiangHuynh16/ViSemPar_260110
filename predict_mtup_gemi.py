import torch
import argparse
import os
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm

def load_model(base_model_path, adapter_path):
    print(f"â³ Loading base model: {base_model_path}")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        device_map="auto",
        attn_implementation="sdpa"
    )
    
    print(f"ğŸ”— Loading adapter: {adapter_path}")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    return model, tokenizer

def create_prompt(text, stage):
    if stage == 1:
        # Prompt cho Stage 1: Text -> Concept Graph (No Variables)
        sys_prompt = "Báº¡n lÃ  má»™t chuyÃªn gia ngÃ´n ngá»¯ há»c vá» cáº¥u trÃºc AMR (Abstract Meaning Representation).\nNhiá»‡m vá»¥: Chuyá»ƒn Ä‘á»•i cÃ¢u tiáº¿ng Viá»‡t Ä‘áº§u vÃ o sang Ä‘á»‹nh dáº¡ng Ä‘á»“ thá»‹ AMR chuáº©n PENMAN.\nYÃªu cáº§u Ä‘áº·c biá»‡t:\n1. KHÃ”NG sá»­ dá»¥ng biáº¿n (variables) Ä‘á»‹nh danh (vÃ­ dá»¥: khÃ´ng dÃ¹ng 't / tÃ´i', chá»‰ dÃ¹ng '(tÃ´i)').\n2. Äáº£m báº£o cáº¥u trÃºc ngoáº·c Ä‘Æ¡n () cÃ¢n báº±ng chÃ­nh xÃ¡c.\n3. Chá»‰ giá»¯ láº¡i cÃ¡c Concept vÃ  Relation (vÃ­ dá»¥: :ARG0, :ARG1, :mod)."
        user_input = f"Input: {text}"
    else:
        # Prompt cho Stage 2: Concept Graph + Text -> Full AMR
        sys_prompt = "Báº¡n lÃ  má»™t chuyÃªn gia gÃ¡n nhÃ£n dá»¯ liá»‡u AMR (Abstract Meaning Representation).\nNhiá»‡m vá»¥: HoÃ n thiá»‡n Ä‘á»“ thá»‹ AMR tá»« cáº¥u trÃºc thÃ´ (chÆ°a cÃ³ biáº¿n) vÃ  cÃ¢u gá»‘c.\n\nYÃªu cáº§u QUAN TRá»ŒNG:\n1. GÃ¡n biáº¿n (variables) Ä‘á»‹nh danh cho má»—i concept (vd: '(tÃ´i)' -> '(t / tÃ´i)').\n2. TÃI Sá»¬ Dá»¤NG BIáº¾N (Re-entrancy): Náº¿u má»™t Ä‘á»‘i tÆ°á»£ng xuáº¥t hiá»‡n nhiá»u láº§n hoáº·c Ä‘Æ°á»£c thay tháº¿ báº±ng Ä‘áº¡i tá»« (anh áº¥y, nÃ³, cáº­u ta...), hÃ£y dÃ¹ng láº¡i biáº¿n Ä‘Ã£ khai bÃ¡o trÆ°á»›c Ä‘Ã³ thay vÃ¬ táº¡o biáº¿n má»›i.\n3. Äáº£m báº£o Ä‘Ãºng Ä‘á»‹nh dáº¡ng PENMAN."
        user_input = f"Input: {text}"
        
    prompt = f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    return prompt

def clean_output(text):
    """HÃ m lÃ m sáº¡ch output Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng bá»‹ lá»—i xuá»‘ng dÃ²ng"""
    # 1. Bá» prompt náº¿u cÃ²n dÃ­nh
    if "assistant\n" in text:
        text = text.split("assistant\n")[-1]
    
    # 2. XÃ³a cÃ¡c token káº¿t thÃºc thá»«a
    text = text.replace("<|im_end|>", "").strip()
    
    # 3. Quan trá»ng: XÃ³a xuá»‘ng dÃ²ng BÃŠN TRONG graph Ä‘á»ƒ nÃ³ náº±m trÃªn 1 dÃ²ng duy nháº¥t
    text = text.replace("\n", " ")
    text = re.sub(r'\s+', ' ', text) # Gá»™p nhiá»u khoáº£ng tráº¯ng thÃ nh 1
    
    return text.strip()

def predict(args):
    # 1. Äá»c Input
    print(f"ğŸ“‚ Reading input file: {args.input_file}")
    with open(args.input_file, 'r', encoding='utf-8') as f:
        lines = [l.strip() for l in f.readlines() if l.strip()]

    inputs = []
    # Logic Ä‘á»c file thÃ´ng minh (giá»¯ nguyÃªn cá»§a báº¡n)
    has_prefix = any(line.startswith("SENT:") or line.startswith("Input:") for line in lines[:5])
    if has_prefix:
        for line in lines:
            if line.startswith("SENT:"): inputs.append(line.replace("SENT:", "").strip())
            elif line.startswith("Input:"): inputs.append(line.replace("Input:", "").strip())
    else:
        inputs = lines

    if not inputs:
        print("âŒ ERROR: No inputs found!")
        return

    print(f"ğŸš€ Generating for {len(inputs)} samples | Stage: {args.stage}")

    # 2. Load Model
    model, tokenizer = load_model(args.base_model, args.adapter_path)
    
    # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
    output_dir = os.path.dirname(args.output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 3. Predict & Write Stream (Ghi ngay láº­p tá»©c)
    with open(args.output_file, 'w', encoding='utf-8') as f_out:
        for text in tqdm(inputs):
            prompt = create_prompt(text, args.stage)
            inputs_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs_ids, 
                    max_new_tokens=1024, # Äá»§ dÃ i cho AMR
                    pad_token_id=tokenizer.eos_token_id,
                    do_sample=False,        # Greedy search Ä‘á»ƒ á»•n Ä‘á»‹nh
                    repetition_penalty=1.2, # TÄƒng lÃªn 1.2 Ä‘á»ƒ trá»‹ bá»‡nh láº·p wz...
                    no_repeat_ngram_size=3  # Cháº·n láº·p cá»¥m tá»«
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # LÃ m sáº¡ch output
            final_res = clean_output(generated_text)
            
            # Náº¿u output rá»—ng do lá»—i model, Ä‘iá»n placeholder
            if not final_res:
                final_res = "(a / amr-empty)"
                
            # Ghi xuá»‘ng file (LuÃ´n thÃªm \n á»Ÿ cuá»‘i)
            f_out.write(final_res + "\n")
            f_out.flush() # Äáº©y dá»¯ liá»‡u xuá»‘ng Ä‘Ä©a ngay

    print(f"âœ… Done! Results saved to {args.output_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-7B-Instruct") # Hoáº·c Ä‘Æ°á»ng dáº«n local cá»§a báº¡n
    parser.add_argument("--adapter_path", type=str, required=True)
    parser.add_argument("--input_file", type=str, required=True)
    parser.add_argument("--output_file", type=str, required=True)
    parser.add_argument("--stage", type=int, default=1)
    args = parser.parse_args()
    predict(args)