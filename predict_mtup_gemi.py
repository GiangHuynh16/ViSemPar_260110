import torch
import argparse
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm

def load_model(base_model_path, adapter_path):
    print(f"â³ Loading base model: {base_model_path}")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        device_map="auto",
        attn_implementation="sdpa"
    )
    
    print(f"ğŸ”— Loading adapter: {adapter_path}")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    return model, tokenizer

def create_prompt(text, stage):
    if stage == 1:
        sys_prompt = "Báº¡n lÃ  má»™t chuyÃªn gia ngÃ´n ngá»¯ há»c vá» cáº¥u trÃºc AMR (Abstract Meaning Representation).\nNhiá»‡m vá»¥: Chuyá»ƒn Ä‘á»•i cÃ¢u tiáº¿ng Viá»‡t Ä‘áº§u vÃ o sang Ä‘á»‹nh dáº¡ng Ä‘á»“ thá»‹ AMR chuáº©n PENMAN.\nYÃªu cáº§u Ä‘áº·c biá»‡t:\n1. KHÃ”NG sá»­ dá»¥ng biáº¿n (variables) Ä‘á»‹nh danh (vÃ­ dá»¥: khÃ´ng dÃ¹ng 't / tÃ´i', chá»‰ dÃ¹ng '(tÃ´i)').\n2. Äáº£m báº£o cáº¥u trÃºc ngoáº·c Ä‘Æ¡n () cÃ¢n báº±ng chÃ­nh xÃ¡c.\n3. Chá»‰ giá»¯ láº¡i cÃ¡c Concept vÃ  Relation (vÃ­ dá»¥: :ARG0, :ARG1, :mod)."
        user_input = f"Input: {text}"
    else:
        sys_prompt = "Báº¡n lÃ  má»™t chuyÃªn gia gÃ¡n nhÃ£n dá»¯ liá»‡u AMR (Abstract Meaning Representation).\nNhiá»‡m vá»¥: HoÃ n thiá»‡n Ä‘á»“ thá»‹ AMR tá»« cáº¥u trÃºc thÃ´ (chÆ°a cÃ³ biáº¿n) vÃ  cÃ¢u gá»‘c.\n\nYÃªu cáº§u QUAN TRá»ŒNG:\n1. GÃ¡n biáº¿n (variables) Ä‘á»‹nh danh cho má»—i concept (vd: '(tÃ´i)' -> '(t / tÃ´i)').\n2. TÃI Sá»¬ Dá»¤NG BIáº¾N (Re-entrancy): Náº¿u má»™t Ä‘á»‘i tÆ°á»£ng xuáº¥t hiá»‡n nhiá»u láº§n hoáº·c Ä‘Æ°á»£c thay tháº¿ báº±ng Ä‘áº¡i tá»« (anh áº¥y, nÃ³, cáº­u ta...), hÃ£y dÃ¹ng láº¡i biáº¿n Ä‘Ã£ khai bÃ¡o trÆ°á»›c Ä‘Ã³ thay vÃ¬ táº¡o biáº¿n má»›i.\n3. Äáº£m báº£o Ä‘Ãºng Ä‘á»‹nh dáº¡ng PENMAN."
        user_input = f"Input: {text}"
        
    prompt = f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    return prompt

def predict(args):
    # 1. Äá»c Input thÃ´ng minh hÆ¡n
    print(f"ğŸ“‚ Reading input file: {args.input_file}")
    with open(args.input_file, 'r', encoding='utf-8') as f:
        lines = [l.strip() for l in f.readlines() if l.strip()]

    inputs = []
    # Kiá»ƒm tra xem file cÃ³ prefix SENT: hay khÃ´ng
    has_prefix = any(line.startswith("SENT:") or line.startswith("Input:") for line in lines[:5])
    
    if has_prefix:
        print("â„¹ï¸ Detected format with prefixes (SENT:/Input:)...")
        for line in lines:
            if line.startswith("SENT:"):
                inputs.append(line.replace("SENT:", "").strip())
            elif line.startswith("Input:"):
                inputs.append(line.replace("Input:", "").strip())
    else:
        print("â„¹ï¸ Detected raw text format. Using full lines.")
        inputs = lines

    if len(inputs) == 0:
        print("âŒ ERROR: No inputs found! Please check your input file format.")
        return

    print(f"ğŸš€ Generating for {len(inputs)} samples...")

    # 2. Load Model
    model, tokenizer = load_model(args.base_model, args.adapter_path)
    
    results = []
    for text in tqdm(inputs):
        prompt = create_prompt(text, args.stage)
        inputs_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs_ids, 
                max_new_tokens=1024,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False, 
                repetition_penalty=1.1
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # TÃ¡ch láº¥y pháº§n output sau chá»¯ assistant
        if "assistant\n" in generated_text:
            response = generated_text.split("assistant\n")[-1].strip()
        else:
            # Fallback náº¿u model khÃ´ng generate Ä‘Ãºng format (hiáº¿m gáº·p)
            response = generated_text.replace(prompt, "").strip()
            
        results.append(response)

    # 3. LÆ°u káº¿t quáº£ (Tá»± táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³)
    output_dir = os.path.dirname(args.output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(args.output_file, 'w', encoding='utf-8') as f:
        for res in results:
            f.write(res + "\n")
            
    print(f"âœ… Done! Results saved to {args.output_file}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument("--adapter_path", type=str, required=True)
    parser.add_argument("--input_file", type=str, required=True)
    parser.add_argument("--output_file", type=str, required=True)
    parser.add_argument("--stage", type=int, default=1)
    args = parser.parse_args()
    predict(args)