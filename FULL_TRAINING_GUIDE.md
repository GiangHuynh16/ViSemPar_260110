# ğŸš€ FULL MTUP TRAINING GUIDE

## âœ… ÄÃ£ Verify

Training Ä‘Ã£ cháº¡y thÃ nh cÃ´ng vá»›i 25 samples (minimal mode). BÃ¢y giá» cÃ³ thá»ƒ cháº¡y full training!

**Settings Ä‘Ã£ verify khÃ´ng OOM:**
- âœ… Batch size: 1
- âœ… Gradient accumulation: 1
- âœ… CPU offload: enabled
- âœ… Bitsandbytes: uninstalled

---

## ğŸ¯ OPTION 1: Full Training Trong Tmux (Khuyáº¿n nghá»‹)

### BÆ°á»›c 1: Táº¡o tmux session

```bash
cd ~/ViSemPar_new1
git pull origin main

# Táº¡o tmux session má»›i
tmux new -s mtup_full
```

### BÆ°á»›c 2: Cháº¡y full training

Trong tmux session:

```bash
conda activate lora_py310
bash RUN_FULL_TRAINING.sh
```

### BÆ°á»›c 3: Detach khá»i tmux

Khi training Ä‘Ã£ báº¯t Ä‘áº§u, nháº¥n:
- `Ctrl+B` rá»“i nháº¥n `D`

Training sáº½ tiáº¿p tá»¥c cháº¡y background.

### BÆ°á»›c 4: Reattach Ä‘á»ƒ xem progress

```bash
tmux attach -t mtup_full
```

### BÆ°á»›c 5: Theo dÃµi tá»« xa (optional)

Má»Ÿ terminal khÃ¡c vÃ  xem logs real-time:

```bash
# Xem training logs
tail -f outputs/logs/mtup_*/events.out.tfevents.*

# Hoáº·c TensorBoard
tensorboard --logdir outputs/logs --port 6006
# Má»Ÿ browser: http://server_ip:6006
```

---

## ğŸ¯ OPTION 2: Cháº¡y Trá»±c Tiáº¿p (KhÃ´ng tmux)

Náº¿u káº¿t ná»‘i SSH stable:

```bash
cd ~/ViSemPar_new1
conda activate lora_py310
git pull origin main
bash RUN_FULL_TRAINING.sh
```

**LÆ°u Ã½**: Náº¿u SSH disconnect, training sáº½ dá»«ng!

---

## ğŸ“Š Training Info

### Dataset
- **Training samples**: ~1200 (full ViAMR dataset)
- **Validation samples**: ~150
- **Epochs**: 10

### Time Estimate
- **Per epoch**: ~20-30 phÃºt
- **Total time**: 3-6 giá»
- **GPU usage**: ~20-21 GB

### Checkpoints
Saved every 250 steps táº¡i:
```
outputs/checkpoints_mtup/
â”œâ”€â”€ checkpoint-250/
â”œâ”€â”€ checkpoint-500/
â”œâ”€â”€ checkpoint-750/
â””â”€â”€ ...
```

### Logs
TensorBoard logs táº¡i:
```
outputs/logs/mtup_YYYYMMDD_HHMMSS/
```

---

## ğŸ” Monitor Progress

### Xem logs trong tmux
```bash
tmux attach -t mtup_full
```

### Xem GPU usage
```bash
# Terminal khÃ¡c
watch -n 1 nvidia-smi
```

### Xem TensorBoard
```bash
tensorboard --logdir outputs/logs --bind_all --port 6006
```

---

## ğŸ›‘ Dá»«ng Training (Náº¿u cáº§n)

### Dá»«ng táº¡m (trong tmux)
- `Ctrl+C` trong tmux session

### Kill tmux session
```bash
tmux kill-session -t mtup_full
```

### Resume tá»« checkpoint
```bash
# Training sáº½ tá»± Ä‘á»™ng resume tá»« checkpoint má»›i nháº¥t
bash RUN_FULL_TRAINING.sh
```

---

## âœ… Sau Khi Training Xong

### 1. TÃ¬m checkpoint tá»‘t nháº¥t

```bash
# List checkpoints
ls -lh outputs/checkpoints_mtup/

# Training args sáº½ chá»n checkpoint cÃ³ lowest eval loss
# Check file: outputs/checkpoints_mtup/checkpoint-XXXX/
```

### 2. Evaluate trÃªn test set

```bash
python3 evaluate_test_data.py
```

### 3. Inference thá»­

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load model
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    device_map="auto",
    torch_dtype="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(
    base_model,
    "outputs/checkpoints_mtup/checkpoint-BEST"  # Thay BEST báº±ng sá»‘
)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")

# Test
text = "Sentence: TÃ´i Äƒn cÆ¡m\n\nTask 1: Generate AMR structure without variables"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=500)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸ†˜ Troubleshooting

### OOM Error
Náº¿u váº«n OOM, giáº£m batch size xuá»‘ng 1:
```bash
# Already using batch_size=1, grad_accum=1
# Náº¿u váº«n OOM, chuyá»ƒn sang model 1.5B:
python3 train_mtup.py --use-case full_training --no-quantize \
  --model-name Qwen/Qwen2.5-1.5B-Instruct \
  --batch-size 1 --grad-accum 1
```

### Bitsandbytes Error
```bash
# Uninstall again
pip uninstall -y bitsandbytes
conda uninstall -y bitsandbytes
```

### Training quÃ¡ cháº­m
- Kiá»ƒm tra GPU usage: `nvidia-smi`
- Náº¿u GPU < 80%, CPU offload Ä‘ang bottleneck
- Giáº£m CPU offload: edit train_mtup.py, tÄƒng max_memory tá»« "20GB" lÃªn "22GB"

---

## ğŸ“ TÃ“M Táº®T

**CHáº Y NGAY:**

```bash
cd ~/ViSemPar_new1
git pull origin main
tmux new -s mtup_full
conda activate lora_py310
bash RUN_FULL_TRAINING.sh
# Ctrl+B, D Ä‘á»ƒ detach
```

**XEM PROGRESS:**

```bash
tmux attach -t mtup_full
```

**Estimated completion time: 3-6 giá»**
