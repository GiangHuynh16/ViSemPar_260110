# Optimization Applied - MTUP Training

**Tá»‘i Æ°u hÃ³a Ä‘Ã£ Ã¡p dá»¥ng trÆ°á»›c khi deploy lÃªn server**

---

## ğŸ¯ **TÃ“M Táº®T OPTIMIZATION**

NgÃ y Ã¡p dá»¥ng: 2024-12-24
Má»¥c tiÃªu: Tá»‘i Æ°u hÃ³a training performance vÃ  accuracy cho MTUP strategy

---

## âœ… **CÃC THAY Äá»”I ÄÃƒ ÃP Dá»¤NG**

### 1. **Training Configuration (config/config_mtup.py)**

#### **Learning Rate: 3e-4 â†’ 2e-4**
```python
# TRÆ¯á»šC:
"learning_rate": 3e-4,              # Slightly higher for smaller models

# SAU (OPTIMIZED):
"learning_rate": 2e-4,              # OPTIMIZED: Lower for stable training
```

**LÃ½ do:**
- 3e-4 quÃ¡ cao cho 3B model â†’ cÃ³ thá»ƒ overshooting
- 2e-4 stable hÆ¡n, convergence mÆ°á»£t mÃ  hÆ¡n
- Best practice cho LoRA fine-tuning vá»›i small models

**Káº¿t quáº£ mong Ä‘á»£i:**
- Training loss á»•n Ä‘á»‹nh hÆ¡n
- TrÃ¡nh divergence
- Better final performance

---

#### **Number of Epochs: 15 â†’ 10**
```python
# TRÆ¯á»šC:
"num_train_epochs": 15,              # Fewer epochs (MTUP learns faster)

# SAU (OPTIMIZED):
"num_train_epochs": 10,              # OPTIMIZED: MTUP converges faster, 10 epochs sufficient
```

**LÃ½ do:**
- MTUP vá»›i explicit supervision â†’ converge nhanh hÆ¡n
- 15 epochs cÃ³ thá»ƒ overfitting trÃªn ~2500 examples
- 10 epochs Ä‘á»§ cho MTUP strategy

**Káº¿t quáº£ mong Ä‘á»£i:**
- Tiáº¿t kiá»‡m ~33% training time
- TrÃ¡nh overfitting
- Better generalization

---

#### **Validation Split: 5% â†’ 10%**
```python
# TRÆ¯á»šC:
"validation_split": 0.05,        # 5% for validation

# SAU (OPTIMIZED):
"validation_split": 0.1,         # OPTIMIZED: 10% for better validation monitoring
```

**LÃ½ do:**
- 5% (~125 examples) quÃ¡ Ã­t Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ reliable
- 10% (~250 examples) cho validation signal tá»‘t hÆ¡n
- Váº«n giá»¯ Ä‘á»§ training data (90% = ~2250 examples)

**Káº¿t quáº£ mong Ä‘á»£i:**
- Validation metrics reliable hÆ¡n
- Better early stopping decisions
- Monitor overfitting tá»‘t hÆ¡n

---

### 2. **Training Use Cases (train_mtup.py)**

#### **Quick Test**
```bash
# KhÃ´ng Ä‘á»•i - váº«n 100 samples, 1 epoch
# ThÃªm: --lr 2e-4 default
```

#### **Fast Iteration: 2 epochs â†’ 3 epochs**
```python
# TRÆ¯á»šC:
# Fast iteration: 500 samples, 2 epochs

# SAU (OPTIMIZED):
# Fast iteration: 500 samples, 3 epochs
args.epochs = args.epochs or 3
args.lr = args.lr or 2e-4
```

**LÃ½ do:**
- 2 epochs chÆ°a Ä‘á»§ Ä‘á»ƒ model há»c tá»‘t
- 3 epochs váº«n nhanh (~30 min) nhÆ°ng accuracy tá»‘t hÆ¡n

---

#### **Full Training: 3 epochs â†’ 10 epochs**
```python
# TRÆ¯á»šC:
# Full training: all data, 3 epochs

# SAU (OPTIMIZED):
# Full training: all data, 10 epochs
args.epochs = args.epochs or 10
args.lr = args.lr or 2e-4
```

**LÃ½ do:**
- 3 epochs quÃ¡ Ã­t cho production model
- 10 epochs optimal cho MTUP (khÃ´ng quÃ¡ nhiá»u, khÃ´ng quÃ¡ Ã­t)
- Consistent vá»›i config default

---

### 3. **Documentation Updates**

#### **QUICK_COMMANDS.md**
- Cáº­p nháº­t training commands vá»›i optimized values
- ThÃªm recommended flag (â­) cho full training
- ThÃªm example cho 7B model training

---

## ğŸ“Š **SO SÃNH TRÆ¯á»šC VÃ€ SAU**

| Metric | TRÆ¯á»šC | SAU (OPTIMIZED) | Improvement |
|--------|-------|-----------------|-------------|
| Learning Rate | 3e-4 | 2e-4 | More stable |
| Epochs (full) | 3 | 10 | Better learning |
| Validation Split | 5% | 10% | Better monitoring |
| Fast Iteration Epochs | 2 | 3 | Better convergence |
| Expected Training Time | ~1h (underfit) | ~2.5h (optimal) | Quality over speed |

---

## ğŸ¯ **Káº¾T QUáº¢ MONG Äá»¢I**

### **Training Metrics:**
- âœ… Training loss: á»”n Ä‘á»‹nh hÆ¡n, giáº£m Ä‘á»u
- âœ… Validation loss: KhÃ´ng fluctuate nhiá»u
- âœ… No divergence/explosion
- âœ… Smooth learning curve

### **Model Performance:**
- **Target SMATCH F1**: 70-80%
- **Realistic vá»›i 3B model**: 68-75%
- **Vá»›i 7B model**: 75-82%

### **Training Time:**
- **Quick test**: ~10 minutes (khÃ´ng Ä‘á»•i)
- **Fast iteration**: ~30-40 minutes (tÄƒng 50% nhÆ°ng quality tá»‘t hÆ¡n)
- **Full training (3B)**: ~2.5 hours (tÄƒng tá»« 1h nhÆ°ng quality tá»‘t hÆ¡n nhiá»u)
- **Full training (7B)**: ~6-7 hours

---

## ğŸ’¡ **KHUYáº¾N NGHá»Š Sá»¬ Dá»¤NG**

### **Láº§n Ä‘áº§u training:**
```bash
# Step 1: Verify pipeline
python3 train_mtup.py --use-case quick_test --show-sample

# Step 2: Fast iteration Ä‘á»ƒ test
python3 train_mtup.py --use-case fast_iteration

# Step 3: Náº¿u fast_iteration OK, cháº¡y full training
tmux new -s amr-training
python3 train_mtup.py --use-case full_training
```

### **Production training (recommended):**
```bash
# 3B model - Fast & Good
python3 train_mtup.py \
  --model qwen2.5-3b \
  --epochs 10 \
  --batch-size 4 \
  --grad-accum 4 \
  --lr 2e-4 \
  --val-split 0.1

# 7B model - Best Accuracy (náº¿u cÃ³ GPU máº¡nh)
python3 train_mtup.py \
  --model qwen2.5-7b \
  --epochs 15 \
  --batch-size 2 \
  --grad-accum 8 \
  --lr 1e-4 \
  --val-split 0.1
```

---

## ğŸ” **MONITORING CHECKLIST**

Khi training, check cÃ¡c metrics nÃ y:

### **During Training:**
- [ ] Training loss giáº£m Ä‘á»u (khÃ´ng cÃ³ spikes)
- [ ] Validation loss track vá»›i training loss
- [ ] No divergence (loss khÃ´ng tÄƒng Ä‘á»™t ngá»™t)
- [ ] GPU utilization ~80-95%
- [ ] No OOM errors

### **After Training:**
- [ ] Final validation loss < 1.0
- [ ] SMATCH F1 > 65% (acceptable)
- [ ] SMATCH F1 > 70% (good)
- [ ] SMATCH F1 > 75% (excellent)
- [ ] Task 1 accuracy > Task 2 accuracy (expected)

---

## ğŸš¨ **TROUBLESHOOTING**

### **Náº¿u validation loss tÄƒng:**
- Giáº£m learning rate: `--lr 1e-4`
- TÄƒng weight decay: Edit config â†’ `weight_decay: 0.02`
- Giáº£m epochs: `--epochs 8`

### **Náº¿u training quÃ¡ cháº­m:**
- DÃ¹ng model nhá» hÆ¡n: `--model qwen2.5-1.5b`
- TÄƒng batch size (náº¿u GPU cho phÃ©p): `--batch-size 8`

### **Náº¿u SMATCH < 65%:**
- TÄƒng epochs: `--epochs 15`
- Thá»­ template khÃ¡c: Edit config â†’ `template_name: "v5_cot"`
- DÃ¹ng model lá»›n hÆ¡n: `--model qwen2.5-7b`

---

## ğŸ“ **CHANGELOG**

### v1.1 (2024-12-24) - Optimization Applied
- âœ… Learning rate: 3e-4 â†’ 2e-4
- âœ… Epochs (full): 3 â†’ 10
- âœ… Validation split: 5% â†’ 10%
- âœ… Fast iteration: 2 â†’ 3 epochs
- âœ… Documentation updated
- âœ… Use case presets optimized

### v1.0 (2024-12-24) - Initial MTUP Implementation
- âœ… MTUP strategy implementation
- âœ… 5 Vietnamese templates
- âœ… Vietnamese character support
- âœ… Multi-model support

---

## ğŸ“ **LÆ¯U Ã QUAN TRá»ŒNG**

1. **KhÃ´ng cáº§n train lÃ¢u vá»›i MTUP:**
   - MTUP vá»›i explicit supervision â†’ converge nhanh
   - 10 epochs Ä‘á»§, khÃ´ng cáº§n 20-30 epochs nhÆ° standard approach

2. **Validation split quan trá»ng:**
   - 10% validation cho reliable metrics
   - Monitor validation loss Ä‘á»ƒ early stopping

3. **Learning rate tháº¥p = stable:**
   - 2e-4 cho 3B model
   - 1e-4 cho 7B model
   - KhÃ´ng nÃªn > 3e-4

4. **MTUP benefits:**
   - Model nhá» (3B) cÃ³ thá»ƒ Ä‘áº¡t performance gáº§n model lá»›n (7B)
   - Training nhanh hÆ¡n 2-3x
   - Easier subtasks â†’ better learning

---

**Táº¥t cáº£ optimizations Ä‘Ã£ Ä‘Æ°á»£c apply vÃ o code.**
**Sáºµn sÃ ng Ä‘á»ƒ pull vá» server vÃ  training!** ğŸš€