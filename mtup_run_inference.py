import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm
import gc
import re
import os

# ================= CONFIG =================
BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"
STAGE1_ADAPTER = "checkpoints/mtup/stage1_v2/final_adapter"
STAGE2_ADAPTER = "checkpoints/mtup/stage2_v2/final_adapter"

INPUT_FILE = "data/public_test" # File ch·ª©a c√¢u input
TEMP_STAGE1_OUT = "evaluation_results/mtup_v2/pred_stage1_skeleton.txt"
FINAL_RAW_OUT = "evaluation_results/mtup_v2/pred_final_raw.txt"

# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
os.makedirs("evaluation_results/mtup_v2", exist_ok=True)

# ================= PROMPTS (PH·∫¢I GI·ªêNG H·ªÜT L√öC TRAIN) =================
def create_prompt_stage1(sentence):
    sys_prompt = """B·∫°n l√† m·ªôt h·ªá th·ªëng ph√¢n t√≠ch ng·ªØ nghƒ©a AMR (Abstract Meaning Representation).
Nhi·ªám v·ª•: Chuy·ªÉn c√¢u ti·∫øng Vi·ªát th√†nh c·∫•u tr√∫c AMR-No-Var (ch∆∞a c√≥ bi·∫øn).
Quy t·∫Øc tuy·ªát ƒë·ªëi:
1. KH√îNG t·ª± t·∫°o bi·∫øn (v√≠ d·ª•: d√πng '(t√¥i)' thay v√¨ '(t / t√¥i)').
2. ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng ngo·∫∑c m·ªü '(' b·∫±ng s·ªë l∆∞·ª£ng ngo·∫∑c ƒë√≥ng ')'.
3. Output ch·ªâ ch·ª©a duy nh·∫•t ƒë·ªì th·ªã AMR, kh√¥ng gi·∫£i th√≠ch th√™m."""
    
    # Format Qwen Chat
    return f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\nC√¢u: {sentence}<|im_end|>\n<|im_start|>assistant\n"

def create_prompt_stage2(sentence, skeleton):
    sys_prompt = """B·∫°n l√† chuy√™n gia g√°n bi·∫øn cho ƒë·ªì th·ªã AMR (AMR Aligner).
Nhi·ªám v·ª•: Th√™m bi·∫øn (variable) v√†o c·∫•u tr√∫c AMR th√¥.
Quy t·∫Øc S·ªêNG C√íN ƒë·ªÉ tr√°nh l·ªói Duplicate Node:
1. M·ªói Concept ch·ªâ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a bi·∫øn M·ªòT l·∫ßn duy nh·∫•t. V√≠ d·ª•: (t / t√¥i).
2. T√ÅI S·ª¨ D·ª§NG (Re-entrancy): N·∫øu concept xu·∫•t hi·ªán l·∫°i, CH·ªà d√πng t√™n bi·∫øn, KH√îNG vi·∫øt l·∫°i concept.
   - SAI: :ARG0 (t / t√¥i) ... :ARG1 (t / t√¥i)
   - ƒê√öNG: :ARG0 (t / t√¥i) ... :ARG1 t
3. H√£y d√πng c√°c ch·ªØ c√°i ƒë·∫ßu l√†m t√™n bi·∫øn (v / vi·∫øt), n·∫øu tr√πng th√¨ th√™m s·ªë (v2 / vi·∫øt)."""
    
    return f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\nC√¢u: {sentence}\nSkeletion: {skeleton}<|im_end|>\n<|im_start|>assistant\n"

# ================= HELPER FUNCTIONS =================
def load_model(adapter_path):
    print(f"üîÑ Loading Adapter: {adapter_path}")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, quantization_config=bnb_config, device_map="auto", attn_implementation="sdpa"
    )
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    return model, tokenizer

def clean_memory(model, tokenizer):
    print("üßπ Cleaning VRAM...")
    del model
    del tokenizer
    gc.collect()
    torch.cuda.empty_cache()

def infer_batch(model, tokenizer, prompts, batch_size=8):
    results = []
    for i in tqdm(range(0, len(prompts), batch_size), desc="Inferencing"):
        batch_prompts = prompts[i:i+batch_size]
        inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=2048).to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=512, 
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False, # Greedy decoding ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh nh·∫•t
                num_beams=1
            )
        
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        
        # C·∫Øt b·ªè ph·∫ßn prompt, ch·ªâ l·∫•y ph·∫ßn output m·ªõi sinh ra
        for j, text in enumerate(decoded):
            # Qwen output th∆∞·ªùng d√≠nh c·∫£ prompt, c·∫ßn split
            # T√¨m keyword "assistant" cu·ªëi c√πng
            parts = text.split("assistant\n")
            if len(parts) > 1:
                result = parts[-1].strip()
            else:
                result = text.strip()
            results.append(result)
    return results

# ================= MAIN PIPELINE =================
def main():
    # 1. ƒê·ªçc d·ªØ li·ªáu Input
    print(f"üìñ Reading input: {INPUT_FILE}")
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        # Gi·∫£ s·ª≠ file public_test ch·ªâ ch·ª©a c√°c c√¢u (m·ªói c√¢u 1 d√≤ng)
        # N·∫øu file c√≥ d·∫°ng SENT: ..., code s·∫Ω strip s·∫°ch
        lines = [line.strip().replace("SENT:", "").strip() for line in f.readlines() if line.strip()]

    # ---------------- STAGE 1 ----------------
    print("\nüöÄ STARTING STAGE 1 (Structure Generation)...")
    model, tokenizer = load_model(STAGE1_ADAPTER)
    tokenizer.padding_side = "left" # Quan tr·ªçng cho batch generation
    
    prompts_s1 = [create_prompt_stage1(sent) for sent in lines]
    skeletons = infer_batch(model, tokenizer, prompts_s1, batch_size=4) # Gi·∫£m batch n·∫øu OOM
    
    # L∆∞u t·∫°m
    with open(TEMP_STAGE1_OUT, 'w', encoding='utf-8') as f:
        for s in skeletons: f.write(s + "\n")
    
    clean_memory(model, tokenizer) # Gi·∫£i ph√≥ng RAM

    # ---------------- STAGE 2 ----------------
    print("\nüöÄ STARTING STAGE 2 (Variable Alignment)...")
    model, tokenizer = load_model(STAGE2_ADAPTER)
    tokenizer.padding_side = "left"

    prompts_s2 = [create_prompt_stage2(sent, skel) for sent, skel in zip(lines, skeletons)]
    final_amrs = infer_batch(model, tokenizer, prompts_s2, batch_size=4)
    
    clean_memory(model, tokenizer)

    # L∆∞u k·∫øt qu·∫£ th√¥
    with open(FINAL_RAW_OUT, 'w', encoding='utf-8') as f:
        for amr in final_amrs:
            f.write(amr + "\n")
            
    print(f"‚úÖ Inference Complete! Raw output saved to: {FINAL_RAW_OUT}")

if __name__ == "__main__":
    main()