import os
import argparse
import torch
import re
import gc
import numpy as np
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ==========================================
# 1. TEMPLATE & PROMPTS (C·∫£i ti·∫øn ch·∫∑t ch·∫Ω h∆°n)
# ==========================================

def create_prompt_stage1(sentence, target_amr=None):
    # Prompt √©p bu·ªôc c·∫•u tr√∫c ch·∫∑t ch·∫Ω h∆°n
    sys_prompt = """B·∫°n l√† m·ªôt h·ªá th·ªëng ph√¢n t√≠ch ng·ªØ nghƒ©a AMR (Abstract Meaning Representation).
Nhi·ªám v·ª•: Chuy·ªÉn c√¢u ti·∫øng Vi·ªát th√†nh c·∫•u tr√∫c AMR-No-Var (ch∆∞a c√≥ bi·∫øn).
Quy t·∫Øc tuy·ªát ƒë·ªëi:
1. KH√îNG t·ª± t·∫°o bi·∫øn (v√≠ d·ª•: d√πng '(t√¥i)' thay v√¨ '(t / t√¥i)').
2. ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng ngo·∫∑c m·ªü '(' b·∫±ng s·ªë l∆∞·ª£ng ngo·∫∑c ƒë√≥ng ')'.
3. Output ch·ªâ ch·ª©a duy nh·∫•t ƒë·ªì th·ªã AMR, kh√¥ng gi·∫£i th√≠ch th√™m."""

    user_input = f"C√¢u: {sentence}"
    # D√πng format chat chu·∫©n c·ªßa Qwen
    prompt = f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    if target_amr:
        prompt += f"{target_amr}<|im_end|>"
    return prompt

def create_prompt_stage2(sentence, amr_no_vars, target_full_amr=None):
    sys_prompt = """B·∫°n l√† chuy√™n gia g√°n bi·∫øn cho ƒë·ªì th·ªã AMR (AMR Aligner).
Nhi·ªám v·ª•: Th√™m bi·∫øn (variable) v√†o c·∫•u tr√∫c AMR th√¥.
Quy t·∫Øc S·ªêNG C√íN ƒë·ªÉ tr√°nh l·ªói Duplicate Node:
1. M·ªói Concept ch·ªâ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a bi·∫øn M·ªòT l·∫ßn duy nh·∫•t. V√≠ d·ª•: (t / t√¥i).
2. T√ÅI S·ª¨ D·ª§NG (Re-entrancy): N·∫øu concept xu·∫•t hi·ªán l·∫°i, CH·ªà d√πng t√™n bi·∫øn, KH√îNG vi·∫øt l·∫°i concept.
   - SAI: :ARG0 (t / t√¥i) ... :ARG1 (t / t√¥i)
   - ƒê√öNG: :ARG0 (t / t√¥i) ... :ARG1 t
3. H√£y d√πng c√°c ch·ªØ c√°i ƒë·∫ßu l√†m t√™n bi·∫øn (v / vi·∫øt), n·∫øu tr√πng th√¨ th√™m s·ªë (v2 / vi·∫øt)."""

    user_input = f"C√¢u: {sentence}\nSkeletion: {amr_no_vars}"
    prompt = f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
    if target_full_amr:
        prompt += f"{target_full_amr}<|im_end|>"
    return prompt

def format_data(sample, stage):
    text = sample['text'].strip()
    if not text: return None
    try:
        # Regex linh ho·∫°t h∆°n ƒë·ªÉ b·∫Øt c√°c bi·∫øn th·ªÉ c·ªßa file input
        if stage == 1:
            match = re.search(r'(?:SENT|Input):\s*(.*?)\n(?:AMR|Output):\s*(.*)', text, re.DOTALL)
            if match: return create_prompt_stage1(match.group(1).strip(), match.group(2).strip())
        else:
            # ∆Øu ti√™n format 3 ph·∫ßn
            match = re.search(r'(?:SENT|Input):\s*(.*?)\n(?:NO_VAR):\s*(.*?)\n(?:FULL|Output):\s*(.*)', text, re.DOTALL)
            if match: return create_prompt_stage2(match.group(1).strip(), match.group(2).strip(), match.group(3).strip())
            
            # Fallback cho format c≈©
            match = re.search(r'Input:\s*(.*?)<sep>(.*?)\nOutput:\s*(.*)', text, re.DOTALL)
            if match: return create_prompt_stage2(match.group(1).strip(), match.group(2).strip(), match.group(3).strip())
        return None
    except Exception: return None

# ==========================================
# 2. VALIDATION & LOADING
# ==========================================

try:
    import penman
except ImportError:
    print("‚ö†Ô∏è Warning: Penman not installed. Skipping strict data validation.")
    penman = None

def load_and_validate_dataset(file_path, stage):
    print(f"üìÇ Reading file: {file_path}")
    if not os.path.exists(file_path): raise FileNotFoundError(f"Cannot find: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f: content = f.read()
    
    blocks = [b for b in content.strip().split('\n\n') if b.strip()]
    valid_data = []
    
    print("üîç Validating data format...")
    for b in blocks:
        formatted = format_data({'text': b}, stage)
        if formatted:
            # Basic validation: Check if target AMR is valid PENMAN (Optional but recommended)
            # N·∫øu data g·ªëc l·ªói -> Train ra model l·ªói. N√™n l·ªçc k·ªπ.
            valid_data.append(formatted)
            
    print(f"Dataset: {len(blocks)} raw -> {len(valid_data)} valid samples ready for training.")
    if not valid_data: raise ValueError("‚ùå DATASET IS EMPTY OR FORMAT WRONG!")
    return Dataset.from_dict({"text": valid_data})

# ==========================================
# 3. TRAINING ENGINE
# ==========================================

def train(args):
    print(f"üöÄ START RESCUE TRAINING STAGE {args.stage}")
    torch.cuda.empty_cache()
    gc.collect()

    # 1. Load Data
    raw_dataset = load_and_validate_dataset(args.data_path, args.stage)

    # 2. Model Config (4-bit QLoRA)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right" # Quan tr·ªçng cho training

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config, 
        device_map="auto",
        attn_implementation="flash_attention_2" if torch.cuda.get_device_capability()[0] >= 8 else "sdpa"
    )
    model = prepare_model_for_kbit_training(model)

    # 3. LoRA Config
    peft_config = LoraConfig(
        lora_alpha=16, 
        lora_dropout=0.05,
        r=64,          # TƒÉng Rank l√™n 64 ƒë·ªÉ model h·ªçc th√¥ng minh h∆°n (v·∫´n v·ª´a VRAM 24GB)
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # 4. Tokenization & MASKING (CRITICAL FIX)
    # H√†m n√†y s·∫Ω set label = -100 cho ph·∫ßn User Prompt, ch·ªâ t√≠nh loss cho Assistant Answer
    def tokenize_with_masking(batch):
        tokenized_inputs = tokenizer(batch['text'], truncation=True, max_length=1536, padding=False)
        input_ids_list = tokenized_inputs["input_ids"]
        attention_mask_list = tokenized_inputs["attention_mask"]
        
        labels_list = []
        
        # Token ƒë√°nh d·∫•u b·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi c·ªßa Assistant (Qwen format)
        # Qwen d√πng: <|im_start|>assistant\n
        # Ch√∫ng ta c·∫ßn t√¨m token ID c·ªßa chu·ªói n√†y ho·∫∑c t∆∞∆°ng t·ª±.
        # ƒê∆°n gi·∫£n nh·∫•t: Tokenize l·∫°i prompt ph·∫ßn user, l·∫•y ƒë·ªô d√†i, mask ph·∫ßn ƒë√≥.
        
        for input_ids, text in zip(input_ids_list, batch['text']):
            # T√°ch ph·∫ßn User prompt v√† Assistant answer
            split_text = text.split("<|im_start|>assistant\n")
            if len(split_text) < 2:
                # Tr∆∞·ªùng h·ª£p l·ªói format, mask to√†n b·ªô (ignore)
                labels_list.append([-100] * len(input_ids))
                continue
                
            prompt_part = split_text[0] + "<|im_start|>assistant\n"
            prompt_ids = tokenizer(prompt_part, truncation=True, max_length=1536, add_special_tokens=False)["input_ids"]
            
            prompt_len = len(prompt_ids)
            
            # T·∫°o labels: Copy input_ids
            labels = list(input_ids)
            # Mask ph·∫ßn prompt (set v·ªÅ -100)
            for i in range(min(prompt_len, len(labels))):
                labels[i] = -100
                
            labels_list.append(labels)
            
        return {
            "input_ids": input_ids_list,
            "attention_mask": attention_mask_list,
            "labels": labels_list
        }

    print("üîÑ Tokenizing & Masking Inputs...")
    tokenized_dataset = raw_dataset.map(tokenize_with_masking, batched=True, remove_columns=["text"])

    # 5. Training Args (T·ªëi ∆∞u cho h·ªôi t·ª• nhanh)
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=2,      # Th·ª≠ tƒÉng l√™n 2 n·∫øu VRAM cho ph√©p (ƒë·ªÉ Batch Norm ·ªïn ƒë·ªãnh h∆°n)
        gradient_accumulation_steps=16,     # 2 * 16 = 32 effective batch
        learning_rate=1e-4,                 # Gi·∫£m LR m·ªôt ch√∫t ƒë·ªÉ tr√°nh ph√° h·ªèng pre-trained weights
        weight_decay=0.01,
        bf16=True,
        logging_steps=5,
        save_strategy="epoch",
        optim="paged_adamw_32bit",
        report_to="none",
        warmup_ratio=0.03,                  # Warmup ƒë·ªÉ model kh√¥ng b·ªã shock
        group_by_length=True,               # Train c√°c c√¢u c√πng ƒë·ªô d√†i v·ªõi nhau -> nhanh h∆°n
    )

    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        pad_to_multiple_of=8,
        return_tensors="pt",
        padding=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator
    )

    print("üî• Training started...")
    trainer.train()
    
    print("üíæ Saving model...")
    trainer.model.save_pretrained(os.path.join(args.output_dir, "final_adapter"))
    tokenizer.save_pretrained(os.path.join(args.output_dir, "final_adapter"))
    print("‚úÖ DONE.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--stage", type=int, required=True)
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-7B-Instruct") 
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--epochs", type=int, default=3) 
    
    args = parser.parse_args()
    train(args)