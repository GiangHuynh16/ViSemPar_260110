# âœ… MTUP Fixed - Ready to Train Checklist

## ðŸ“‹ Pre-Training Verification

### âœ… Implementation Files Created

- [x] [config/prompt_templates_fixed.py](config/prompt_templates_fixed.py) - Minimal prompts with Penman examples
- [x] [config/config_mtup_fixed.py](config/config_mtup_fixed.py) - Training configuration (2 epochs, masking)
- [x] [train_mtup_fixed.py](train_mtup_fixed.py) - Training script with instruction masking
- [x] [predict_mtup_fixed.py](predict_mtup_fixed.py) - Two-stage inference pipeline
- [x] [preprocess_mtup.py](preprocess_mtup.py) - Data preprocessing script

### âœ… Helper Scripts Created

- [x] [TRAIN_MTUP_FIXED.sh](TRAIN_MTUP_FIXED.sh) - Training wrapper
- [x] [TEST_MTUP_FIXED.sh](TEST_MTUP_FIXED.sh) - Single example test
- [x] [EVALUATE_MTUP_CHECKPOINTS.sh](EVALUATE_MTUP_CHECKPOINTS.sh) - Checkpoint evaluation
- [x] All scripts are executable (`chmod +x`)

### âœ… Documentation Created

- [x] [START_HERE_MTUP.md](START_HERE_MTUP.md) - Complete quick start guide
- [x] [MTUP_FIXED_SUMMARY.md](MTUP_FIXED_SUMMARY.md) - Technical summary
- [x] [BASELINE_VS_MTUP_COMPARISON.md](BASELINE_VS_MTUP_COMPARISON.md) - Comparison table
- [x] [MTUP_READY_TO_TRAIN.md](MTUP_READY_TO_TRAIN.md) - This checklist

---

## ðŸ” Code Quality Checks

### âœ… Instruction Masking Implementation

**File:** [train_mtup_fixed.py:156-177](train_mtup_fixed.py#L156-L177)

```python
# Verified: Encodes separately WITHOUT special tokens
instruction_ids = self.tokenizer.encode(instruction, add_special_tokens=False)
target_ids = self.tokenizer.encode(target, add_special_tokens=False)
eos_ids = self.tokenizer.encode(self.tokenizer.eos_token, add_special_tokens=False)

# Verified: Masks instruction tokens
labels = input_ids.copy()
for i in range(len(instruction_ids)):
    labels[i] = -100  # Only train on target
```

âœ… **Status:** Instruction masking correctly implemented (same as Baseline)

---

### âœ… Prompt Template Verification

**File:** [config/prompt_templates_fixed.py](config/prompt_templates_fixed.py)

**Checklist:**
- [x] Emphasizes "chuáº©n PENMAN" (Penman standard)
- [x] Includes 1 clear Penman example
- [x] Shows both stages (no vars â†’ with vars)
- [x] Clear extraction markers: "AMR khÃ´ng biáº¿n:" and "AMR chuáº©n PENMAN:"
- [x] Minimal length (~10 lines vs old 20+)

âœ… **Status:** Prompt template meets all requirements

---

### âœ… Training Configuration Verification

**File:** [config/config_mtup_fixed.py](config/config_mtup_fixed.py)

**Critical settings:**
- [x] `num_train_epochs`: 2 (prevent overfitting)
- [x] `save_steps`: 100 (early stopping)
- [x] `bf16`: True (bfloat16 precision)
- [x] `use_instruction_masking`: True (NEW)
- [x] LoRA config matches Baseline (r=64, alpha=128)

âœ… **Status:** Configuration matches Baseline's successful approach

---

### âœ… Two-Stage Inference Verification

**File:** [predict_mtup_fixed.py](predict_mtup_fixed.py)

**Checklist:**
- [x] `generate_step1()`: Sentence â†’ AMR without vars
- [x] `generate_step2()`: AMR no vars â†’ Penman AMR
- [x] `extract_amr_step1()`: Extracts from "AMR khÃ´ng biáº¿n:" marker
- [x] `extract_amr_step2()`: Extracts from "AMR chuáº©n PENMAN:" marker
- [x] Parenthesis balance checking on accumulated text
- [x] Validation: balance, duplicates, non-empty

âœ… **Status:** Two-stage pipeline correctly implemented

---

## ðŸ“¦ Data Preparation

### Before Training, You Need To:

1. **Preprocess Training Data** (5 minutes)

```bash
python3 preprocess_mtup.py \
    --input data/train_amr_1.txt \
    --output data/train_amr_mtup_preprocessed.txt \
    --validate
```

**Expected output:**
```
ðŸ“Š Statistics:
  Total examples: 1090
  Output file: data/train_amr_mtup_preprocessed.txt
  File size: ~500 KB

ðŸ” Validating AMRs...
  âœ… All 1090 AMRs are valid!
```

**Verify:**
- [ ] Output file created: `data/train_amr_mtup_preprocessed.txt`
- [ ] File size: ~500 KB
- [ ] All AMRs validated
- [ ] No errors during preprocessing

---

## ðŸ–¥ï¸ Environment Check

### GPU Requirements

**Check VRAM availability:**

```bash
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader
```

**Requirements:**
- [x] GPU: NVIDIA with â‰¥24GB VRAM (A6000, RTX 3090, etc.)
- [x] CUDA: â‰¥11.8
- [x] Free VRAM: â‰¥26GB for training

---

### Python Dependencies

**Verify installed:**

```bash
python3 -c "
import torch
import transformers
import peft
import bitsandbytes

print(f'PyTorch: {torch.__version__}')
print(f'Transformers: {transformers.__version__}')
print(f'PEFT: {peft.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
"
```

**Expected:**
```
PyTorch: 2.0.1+cu118
Transformers: 4.36.2
PEFT: 0.7.1
CUDA available: True
GPU: NVIDIA A6000
```

**Requirements:**
- [x] PyTorch â‰¥2.0
- [x] Transformers â‰¥4.36
- [x] PEFT â‰¥0.7
- [x] CUDA available

---

## ðŸš€ Ready to Train!

### Final Checklist

- [ ] âœ… All implementation files created
- [ ] âœ… Code quality verified (masking, prompts, inference)
- [ ] âœ… Training data preprocessed
- [ ] âœ… GPU available (â‰¥24GB VRAM)
- [ ] âœ… Dependencies installed
- [ ] âœ… Disk space: â‰¥30GB free

### Start Training

```bash
bash TRAIN_MTUP_FIXED.sh
```

**What to expect:**
```
================================================================================
MTUP FIXED TRAINING
================================================================================

ðŸŽ¯ Key Improvements from Baseline:
  âœ… Instruction masking (train only on final AMR)
  âœ… Minimal prompt with Penman examples
  âœ… 2 epochs (prevent overfitting)
  âœ… Save every 100 steps (early stopping)
  âœ… bfloat16 precision

ðŸ“Š Expected Results:
  - Structural validity: >90% (baseline achieved 91.3%)
  - SMATCH F1: ~0.50-0.55 (hypothesis: MTUP improves over baseline's 0.47)
  - Training time: ~4 hours (2-stage model, 2 epochs)

ðŸš€ Start training? (y/n)
```

---

## ðŸ“Š During Training

### Monitor Progress

**Terminal 1: Training**
```bash
bash TRAIN_MTUP_FIXED.sh
```

**Terminal 2: Logs**
```bash
tail -f logs/training_mtup_fixed_*.log
```

**Expected log output:**
```
Step 100: loss=0.847
Step 200: loss=0.623
Step 300: loss=0.512
Step 400: loss=0.445
...
Step 1500: loss=0.198
Step 1600: loss=0.187
```

**Good signs:**
- âœ… Loss decreases steadily
- âœ… No NaN or Inf values
- âœ… Memory usage stable (~26GB)
- âœ… Checkpoints saving every 100 steps

**Warning signs:**
- âŒ Loss not decreasing after 500 steps
- âŒ OOM errors
- âŒ Loss becomes NaN
- âŒ No checkpoints saved

---

### Training Timeline

| Time | Step | Expected Loss | Notes |
|------|------|---------------|-------|
| 0:00 | 0 | ~2.5 | Initial |
| 0:30 | 100 | ~0.8 | First checkpoint |
| 1:00 | 200 | ~0.6 | Loss dropping |
| 1:30 | 300 | ~0.5 | Good progress |
| 2:00 | 400-800 | ~0.4-0.3 | Epoch 1 complete |
| 3:00 | 800-1200 | ~0.25-0.20 | Epoch 2 progress |
| 4:00 | 1600 | ~0.18 | Training complete |

---

## ðŸ§ª After Training

### Step 1: Quick Test (2 minutes)

```bash
bash TEST_MTUP_FIXED.sh
```

**Check:**
- [ ] Model loads successfully
- [ ] Two-stage generation works
- [ ] Output is valid Penman format
- [ ] Not generating explanations

---

### Step 2: Evaluate All Checkpoints (30 minutes)

```bash
bash EVALUATE_MTUP_CHECKPOINTS.sh outputs/mtup_fixed_YYYYMMDD_HHMMSS
```

**Expected output:**
```
Checkpoint       Valid_AMRs  Total  Validity_Percentage
checkpoint-100   42          50     84.0
checkpoint-200   45          50     90.0
checkpoint-300   46          50     92.0  â† Best
checkpoint-400   44          50     88.0
...

ðŸ† Best checkpoint:
  checkpoint-300 with 92.0% validity (46/50 valid AMRs)
```

**Select best checkpoint** (highest validity, typically 300-500)

---

### Step 3: Test on Full Dataset (1 hour)

```bash
python3 predict_mtup_fixed.py \
    --model outputs/mtup_fixed_YYYYMMDD/checkpoint-300 \
    --test-file data/public_test.txt \
    --output evaluation_results/mtup_predictions.txt \
    --verbose
```

**Check:**
- [ ] All 150 predictions generated
- [ ] Structural validity >85%
- [ ] Output saved to file

---

### Step 4: Calculate SMATCH (5 minutes)

```bash
# Filter valid AMRs
python3 filter_valid_amrs.py \
    --predictions evaluation_results/mtup_predictions.txt \
    --ground-truth data/public_test_ground_truth.txt \
    --output-pred evaluation_results/mtup_valid.txt \
    --output-gold evaluation_results/gold_valid.txt

# Calculate SMATCH
python -m smatch -f \
    evaluation_results/mtup_valid.txt \
    evaluation_results/gold_valid.txt \
    --significant 4
```

**Expected output:**
```
F-score: 0.52
Precision: 0.54
Recall: 0.50
```

---

## ðŸ“ˆ Results Comparison

### Fill in After Training

| Metric | Baseline | MTUP | Delta | Winner |
|--------|----------|------|-------|--------|
| **SMATCH F1** | 0.47 | ??? | ??? | ??? |
| **Structural Validity** | 91.3% | ??? | ??? | ??? |
| **Invalid AMRs** | 13/150 | ??? | ??? | ??? |
| **Training Time** | 2.5h | ~4h | +1.5h | Baseline |
| **Inference Speed** | 5/sec | ~2.5/sec | -50% | Baseline |

**Overall winner:** TBD based on F1 and validity

---

## ðŸŽ¯ Success Criteria

### Minimum Acceptable
- [ ] âœ… Training completes without errors
- [ ] âœ… Loss decreases steadily
- [ ] âœ… Structural validity >85%
- [ ] âœ… SMATCH F1 >0.45

### Target (Hypothesis)
- [ ] ðŸŽ¯ Structural validity >90%
- [ ] ðŸŽ¯ SMATCH F1 >0.50
- [ ] ðŸŽ¯ Improvement over Baseline

### Excellent
- [ ] ðŸ† Structural validity >92%
- [ ] ðŸ† SMATCH F1 >0.52
- [ ] ðŸ† Clear advantage over Baseline

---

## ðŸ› Troubleshooting

### Issue: OOM during training

**Solution 1:** Reduce batch size
```python
# In config/config_mtup_fixed.py
TRAINING_CONFIG = {
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32,  # Increase from 16
}
```

**Solution 2:** Reduce LoRA rank
```python
LORA_CONFIG = {
    "r": 32,            # Reduce from 64
    "lora_alpha": 64,
}
```

---

### Issue: Loss not decreasing

**Check:**
1. Instruction masking enabled: `grep use_instruction_masking config/config_mtup_fixed.py`
2. Data preprocessed correctly: `head -20 data/train_amr_mtup_preprocessed.txt`
3. Learning rate not too high: Should be `2e-4`

**Solution:** Verify preprocessed data format matches expected

---

### Issue: Invalid AMRs (>20%)

**Check:**
1. Using best checkpoint, not latest
2. Prompt has Penman examples
3. Extraction logic correct

**Solution:** Try different checkpoint (300-500 usually best)

---

## ðŸ“š Documentation After Results

### Update These Files

1. **Thesis Section 4.5** - MTUP method and results
2. **README.md** - Add MTUP results
3. **GitHub** - Push final model and results

### Comparison Table for Thesis

```markdown
| Method | F1 | Validity | Training | Inference | Parameters |
|--------|-----|----------|----------|-----------|------------|
| Baseline | 0.47 | 91.3% | 2.5h | 5/sec | 11M (0.15%) |
| MTUP | 0.XX | XX.X% | 4h | 2.5/sec | 11M (0.15%) |
```

---

## âœ… Final Status

**All prerequisites met:** âœ… YES

**Ready to train:** âœ… YES

**Next step:** Run `bash TRAIN_MTUP_FIXED.sh`

**Estimated completion:** ~4 hours from start

---

## ðŸŽ¯ Quick Reference

**Training:**
```bash
bash TRAIN_MTUP_FIXED.sh
```

**Testing:**
```bash
bash TEST_MTUP_FIXED.sh
```

**Evaluation:**
```bash
bash EVALUATE_MTUP_CHECKPOINTS.sh outputs/mtup_fixed_*/
```

**Full documentation:** [START_HERE_MTUP.md](START_HERE_MTUP.md)

---

**Good luck! ðŸš€**

Remember:
- Monitor training logs
- Training takes ~4 hours
- Select best checkpoint (not latest!)
- Compare with Baseline (F1=0.47)
